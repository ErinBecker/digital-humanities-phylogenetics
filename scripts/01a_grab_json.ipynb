{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing ORACC Data from corpus.json\n",
    "by Niek Veldhuis\n",
    "UC Berkeley\n",
    "\n",
    "\n",
    "# TODO\n",
    "* check that COFs are treated properly\n",
    "* check that lines that continue into the next line (as in bilinguals) are captured completely. Such lines are indicated in the json by the the addition of 'l' (lower case L) to the reference (.ref).\n",
    "* add definition of fields to list in Introduction\n",
    "\n",
    "# Note\n",
    "Currently the code will fetch a large zip file from ORACC, download it, extract certain files from the zip file and parse those. The zip file contains all data that belong to an ORACC project or sub-project. Since one may run this notebook several times for collecting data from the same project, this may not be the best process (the download will take place every time). Move the download process to a separate notebook, preceding the current one.\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Purpose of the code is to download [ORACC](http://oracc.org) (Open Richly Annotated Cuneiform Corpus) JSON files that contain textual data and produce a `.csv` file in the directory `data/raw` with the relevant data for use in the phylogenetics project. The JSON files contain all the transliteration and lemmatization data of an ORACC project (metadata are made available in a separate `.json` file). For an introduction to the various ORACC JSON files see the [ORACC Open Data](http://oracc.org/doc/opendata) page.\n",
    "\n",
    "The resulting data file includes various elements of the ORACC data structure. The current code will output a file with the following fields: \n",
    "\n",
    "* id_line\n",
    "* label\n",
    "* lemma\n",
    "* base\n",
    "* extent\n",
    "* scope\n",
    "\n",
    "The fields `extent` and `scope` capture the number of missing lines or columns.\n",
    "\n",
    "The selection of fields may be adjusted with standard `Pandas` functions.\n",
    "\n",
    "## Notes\n",
    "The current version of the script works with the `requests` library.  \n",
    "\n",
    "This notebook is written for **Python 3.5** with **Pandas 0.19** and **requests 2.18.1**.\n",
    "\n",
    "The notebook was written for the [Digital Humanities Phylogenetics](https://github.com/ErinBecker/digital-humanities-phylogenetics) project with Erin Becker of [Data Carpentry](http://www.datacarpentry.org). The particular data selection and data manipulation performed in this notebook are inspired by the needs of that project (for instance, non-Sumerian words are filtered out). It should be fairly easy to adapt the notebook to the purposes of any other project that wishes to use [ORACC](http://oracc.org) data.\n",
    "\n",
    "## Licensing\n",
    "This notebook may be downloaded, used and adapted without any restrictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   \n",
    "import requests\n",
    "import zipfile\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input List of Text IDs\n",
    "Identify a list of text IDs (P, Q, and X numbers) in the directory `data/text_ids`. The IDs are six-digit P, Q, or X numbers preceded by a project abbreviation in the format 'PROJECT/P######' or 'PROJECT/SUBPROJECT/Q######'. Each number may be followed by a start and/or stop label. For example:\n",
    "\n",
    "* etcsri/Q001203\n",
    "* rinap/rinap1/Q003421\n",
    "* dcclt/P117395 r i 23 - r ii 3\n",
    "* dcclt/P453267 - r iv 35'\n",
    "* dcclt/P236734 o ii 12 -\n",
    "\n",
    "The list should be created with a flat text editor such as Textedit or Emacs (not in a word processor such as MS Word), and the filename should end in `.txt`. The labels should copy exactly the line lables as used in the online [ORACC](http://oracc.org) editions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: Q39_par.txt\n"
     ]
    }
   ],
   "source": [
    "filename = input('Filename: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textids = '../data/text_ids/' + filename\n",
    "with open(textids, 'r') as f:\n",
    "    pqxnos = f.read().splitlines()\n",
    "pqxnos = [no.strip() for no in pqxnos] # strip spaces left and right\n",
    "nos_labels = [no.split(' ', 1) if \" \" in no else [no, '-'] for no in pqxnos] #separate ID from labels\n",
    "for label in nos_labels:  # split labels into start label and stop label\n",
    "    label[1] = label[1].split('-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse\n",
    "\n",
    "`Parsejson()` takes as second argument a logical variable. If \"True\" the parser starts with the first word. If \"False\" the parser starts when it gets to `startlabel`. The parser stops when it gets to `endlabel`. `Label`, `startlabel` and `stoplabel` are stored in the dictionary `labels` outside of the function.\n",
    "The list `dollar_keys` (also outside of the function) stores the relevant field names when capturing line breaks etc. \n",
    "\n",
    "Words not only include lemmatized words, but also unlemmatized and unlemmatizable words (such as breaks).\n",
    "\n",
    "The resulting dictionary includes keys such as `lang` (for language), `guideword`, `sense`, etc. - all the elements that define an [ORACC](http://oracc.org) signature. The dictionary also includes the key `id_word` (a sequential number for each word in each line) which has the format `TextID.LineID.WordID` - in other words, line and text ID can be derived from it. This allows the user to reassemble a text in the original word and line order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsejson(text, parameters):\n",
    "    for JSONobject in text[\"cdl\"]:\n",
    "        if \"cdl\" in JSONobject: \n",
    "            parsejson(JSONobject, parameters)\n",
    "        if \"label\" in JSONobject:\n",
    "            parameters[\"label\"] = JSONobject[\"label\"]\n",
    "        if parameters[\"label\"] == parameters[\"startlabel\"]:\n",
    "            parameters[\"keep\"] = True\n",
    "        if parameters[\"label\"] == parameters[\"endlabel\"]:\n",
    "            parameters[\"keep\"] = False\n",
    "        if parameters[\"keep\"] == True or parameters[\"label\"] == parameters[\"endlabel\"]: # the \"or\" statement ensures that the line\n",
    "            if \"f\" in JSONobject:             # corresponding to the endlabel is included.\n",
    "                lemma = JSONobject[\"f\"]\n",
    "                lemma[\"id_word\"] = JSONobject[\"ref\"]\n",
    "                lemma[\"label\"] = parameters[\"label\"]\n",
    "                lemma[\"id_text\"] = parameters[\"id_text\"]\n",
    "                lemm_l.append(lemma)\n",
    "            if \"strict\" in JSONobject and JSONobject[\"strict\"] == \"1\":\n",
    "                lemma = {key: JSONobject[key] for key in dollar_keys}\n",
    "                lemma[\"id_word\"] = JSONobject[\"ref\"] + \".0\"\n",
    "                lemma[\"id_text\"] = parameters[\"id_text\"]\n",
    "                lemm_l.append(lemma)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call the Parser Function for Each Textid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/138 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3ac5978be565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"keep\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://oracc.museum.upenn.edu/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mproject\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/corpusjson/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtextid\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mparsejson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    848\u001b[0m                     \u001b[0;31m# used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "lemm_l = []\n",
    "dollar_keys = [\"extent\", \"scope\", \"state\"]\n",
    "for pqx in tqdm.tqdm(nos_labels):\n",
    "    project = pqx[0][:-8].lower()\n",
    "    textid = pqx[0][-7:].upper()\n",
    "    parameters = {\"startlabel\":pqx[1][0].strip(), \"endlabel\":pqx[1][1].strip(), \"label\":None,\n",
    "                 \"keep\": False, \"id_text\": project + '/' + textid}\n",
    "    if parameters[\"startlabel\"] == \"\":\n",
    "        parameters[\"keep\"] = True\n",
    "    else:\n",
    "        parameters[\"keep\"] = False\n",
    "    url = \"http://oracc.museum.upenn.edu/\" + project + \"/corpusjson/\" + textid + \".json\"  \n",
    "    r = requests.get(url).json()\n",
    "    try:\n",
    "        parsejson(r, parameters)\n",
    "    except:\n",
    "        print(url + ' is not available or not complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the Data into a DataFrame\n",
    "\n",
    "If a text has no breakage information in the form of `$ 1 line broken` (etc.) the fields `extent`, `scope`, and `state` do not exist. The fields `extent` and `scope` are referenced in the code below. After creating the dataframe the existence of these two fields is checked - if they do not exist, empty columns are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words = pd.DataFrame(lemm_l)\n",
    "if not 'extent' in words.columns:\n",
    "    words['extent'] = ''\n",
    "if not 'scope' in words.columns:\n",
    "    words['scope'] = ''\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Spaces and Commas from Guide Word and Sense\n",
    "Spaces in Guide Word and Sense may cause trouble in computational methods in tokenization, or when saved in Comma Separated Values format. All spaces and commas are replaced by hyphens or nothing, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words.fillna('') # first replace Missing Values by empty string\n",
    "words['sense'] = [x.replace(' ', '-') for x in words['sense']]\n",
    "words['sense'] = [x.replace(',', '') for x in words['sense']]\n",
    "words['gw'] = [x.replace(' ', '-') for x in words['gw']]\n",
    "words['gw'] = [x.replace(',', '') for x in words['gw']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns in the resulting DataFrame correspond to the elements of a full [ORACC](http://oracc.org) signature, plus information about text, line, and word ids:\n",
    "* base (Sumerian only)\n",
    "* cf (Citation Form)\n",
    "* cont (continuation of the base; Sumerian only)\n",
    "* epos (Effective Part of Speech)\n",
    "* form (transliteration, omitting all flags such as indication of breakage)\n",
    "* gw (Guide Word: main or first translation in standard dictionary)\n",
    "* id_line (a line ID that begins with the six-digit P, Q, or X number of the text)\n",
    "* id_text (six-digit P, Q, or X number)\n",
    "* id_word (word ID that begins with the ID number of the line)\n",
    "* label (traditional line number in the form o ii 2' (obverse column 2 line 2'), etc.)\n",
    "* lang (language code, including sux, sux-x-emegir, sux-x-emesal, akk, akk-x-stdbab, etc)\n",
    "* morph (Morphology; Sumerian only)\n",
    "* norm (Normalization: Akkadian)\n",
    "* norm0 (Normalization: Sumerian)\n",
    "* pos (Part of Speech)\n",
    "* sense (contextual meaning)\n",
    "* signature (full ORACC signature)\n",
    "\n",
    "Not all data elements (columns) are available for all words. Sumerian words never have a `norm`, Akkadian words do not have `norm0`, `base`, `cont`, or `morph`. Most data elements are only present when the word is lemmatized; only `lang`, `form`, `pos`, `id_word`, `id_line`, and `id_text` should always be there. An unlemmatized word has `pos` 'X' (for unknown). Broken words have `pos` 'u' (for 'unlemmatizable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulate\n",
    "The columns may be manipulated with standard Pandas methods to create the desired output. By way of example, the following code will create a column `lemma` with the format **cf[gw]pos** (for instance **lugal[king]N**). For words that have no lemmatization, `lemma` equals `form`. Only Sumerian words are allowed (and thus `lang` can be omitted) and in addition to the column `lemma` the column `base` is preserved; words that have no lemmatization take `form` as their base. Words and bases are concatenated to lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove  non-Sumerian words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = ['sux', ''] # note that 'lang' is empty in entries that indicate damage\n",
    "words = words.loc[words['lang'].str[:3].isin(lang)].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Lemma Column and Adjust Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['lemma'] = words['cf'] # first element of lemma is the citation form\n",
    "words['lemma'] = [words['lemma'][i] + '[' + words['gw'][i] \n",
    "                     + ']' + words['pos'][i] \n",
    "                     if not words['lemma'][i] == '' \n",
    "                     else words['form'][i] +'[NA]NA' for i in range(len(words))]\n",
    "words['lemma'] = [lemma if not lemma == '[NA]NA' else '' for lemma in words['lemma'] ]\n",
    "words['base'] = [words['base'][i] if not words['base'][i] == '' \n",
    "                 or words['label'][i] == '' else words['form'][i] \n",
    "                 for i in range(len(words))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by Line\n",
    "Create `id_line` and `id_text` from `id_word`. The field `id_word` has the format `Q000039.76.1` (first word in line 76 of Q000039). The corresponding `id_line` is `76` (integer), `id_text` is `Q000039`. \n",
    "\n",
    "`id_line` is an integer that will keep the lines in proper order (`id_word` and `id_text` are strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['id_line'] = [int(wordid[wordid.find('.')+1:wordid.rfind('.')]) for wordid in words['id_word']]\n",
    "words['id_text'] = [wordid[:7] for wordid in words['id_word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = words.groupby([words['id_text'], words['id_line'], words['label']]).agg({\n",
    "        'lemma': ' '.join,\n",
    "        'base': ' '.join,\n",
    "        'extent': ''.join, \n",
    "        'scope': ''.join\n",
    "    }).reset_index()\n",
    "df        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save in CSV Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../data/raw/' + filename[:-4] + '.csv'\n",
    "print('saving ' + filename)\n",
    "with open(filename, 'w') as w:\n",
    "    df.to_csv(w, encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
