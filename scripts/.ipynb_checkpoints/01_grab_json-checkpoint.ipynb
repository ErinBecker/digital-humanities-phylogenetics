{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing ORACC Data from corpus.json\n",
    "by Niek Veldhuis\n",
    "UC Berkeley\n",
    "\n",
    "March-June 2017\n",
    "\n",
    "# TODO\n",
    "* check that COFs are treated properly\n",
    "* check that lines that continue into the next line (as in bilinguals) are captured completely. Such lines are indicated in the json by the the addition of 'l' (lower case L) to the reference (.ref).\n",
    "* add definition of fields to list in Introduction\n",
    "\n",
    "# Note\n",
    "Currently the code will fetch a large zip file from ORACC, download it, extract certain files from the zip file and parse those. The zip file contains all data that belong to an ORACC project or sub-project. Since one may run this notebook several times for collecting data from the same project, this may not be the best process (the download will take place every time). Move the download process to a separate notebook, preceding the current one.\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Purpose of the code is to download [ORACC](http://oracc.org) (Open Richly Annotated Cuneiform Corpus) JSON files that contain textual data and produce a `.csv` file in the directory `data/raw` with the relevant data for use in the phylogenetics project. The JSON files contain all the transliteration and lemmatization data of an ORACC project (metadata are made available in a separate `.json` file). For an introduction to the various ORACC JSON files see the [ORACC Open Data](http://oracc.org/doc/opendata) page.\n",
    "\n",
    "The resulting data file includes various elements of the ORACC data structure. The current code will output a file with the following fields: \n",
    "\n",
    "* id_line\n",
    "* label\n",
    "* lemma\n",
    "* base\n",
    "* extent\n",
    "* scope\n",
    "\n",
    "The fields `extent` and `scope` capture the number of missing lines or columns.\n",
    "\n",
    "The selection of fields may be adjusted with standard `Pandas` functions.\n",
    "\n",
    "## Notes\n",
    "The current version of the script works with the `ijson` library. Documentation for [ijson](https://www.dataquest.io/blog/python-json-tutorial/), unfortunately, is extremely brief. \n",
    "\n",
    "This notebook is written for **Python 3.5** with **Pandas 0.19** and **ijson 2.3**.\n",
    "\n",
    "The notebook was written for the [Digital Humanities Phylogenetics](https://github.com/ErinBecker/digital-humanities-phylogenetics) project with Erin Becker of [Data Carpentry](http://www.datacarpentry.org). The particular data selection and data manipulation performed in this notebook are inspired by the needs of that project (for instance, non-Sumerian words are filtered out). It should be fairly easy to adapt the notebook to the purposes of any other project that wishes to use [ORACC](http://oracc.org) data.\n",
    "\n",
    "## Licensing\n",
    "This notebook may be downloaded, used and adapted without any restrictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd   \n",
    "import ijson\n",
    "import urllib.request\n",
    "import re\n",
    "import zipfile\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input List of Text IDs\n",
    "Identify a list of text IDs (P, Q, and X numbers) in the directory `data/text_ids`. The IDs are six-digit P, Q, or X numbers preceded by a project abbreviation in the format 'PROJECT/P######' or 'PROJECT/SUBPROJECT/Q######'. For example:\n",
    "* dcclt/P117395\n",
    "* etcsri/Q001203\n",
    "* rinap/rinap1/Q003421\n",
    "\n",
    "The list should be created with a flat text editor such as Textedit or Emacs (not in a word processor such as MS Word), and the filename should end in `.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = input('Filename: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textids = '../data/text_ids/' + filename\n",
    "with open(textids, 'r') as f:\n",
    "    pqxnos = f.readlines()\n",
    "pqxnos = [x.strip() for x in pqxnos]\n",
    "projects = [x[:-8] for x in pqxnos]\n",
    "projects = list(set(projects))\n",
    "pqxnos[:5], projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download `json.zip`\n",
    "For each project from which files are to be processed download the entire project (all the json files) in `http://oracc.museum.upenn.edu/PROJECT/json.zip`. For larger projects (such as [DCCLT](http://oracc.org/dcclt)) the `json.zip` may be 25Mb or more. Downloading may take some time and it may be necessary to chunk the downloading process. For the chunking code see instructions and explanations [here](https://www.smallsurething.com/how-to-read-a-file-properly-in-python/).\n",
    "\n",
    "Although downloading the entire zip file is time consuming, it will make processing the individual files much more efficient and the code is less likely to break due to interruption in connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for project in projects:\n",
    "    url = \"http://oracc.museum.upenn.edu/\" + project + \"/json.zip\"\n",
    "    file = '../data/json/' + project.replace('/', '_') + '_json.zip'\n",
    "    print(\"Downloading \" + url + \" saving as \" + file)\n",
    "    response = urllib.request.urlopen(url)\n",
    "    CHUNK = 16 * 1024\n",
    "    with open(file, 'wb') as f:\n",
    "        for chunk in tqdm.tqdm(iter(lambda: response.read(CHUNK), b'')):\n",
    "            f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract\n",
    "Extract from the `json.zip` the individual `.json` files enumerated in the list of text IDs. All files are extracted to a directory called `data/json/corpusjson`. If a list of text IDs has the same P number multiple times (e.g. if editions of the same text exist in multiple projects), the file will be overwritten and only one instance of that P number will be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_dir = '../data/json'\n",
    "for no in pqxnos:\n",
    "    project = no[:-8]\n",
    "    pno = no[-7:]\n",
    "    zip_file = \"../data/json/\" + project.replace('/', '_') + \"_json.zip\"\n",
    "    with zipfile.ZipFile(zip_file,\"r\") as zip_ref:\n",
    "        try:\n",
    "            file = 'corpusjson/' + pno + '.json'\n",
    "            zip_ref.extract(file, target_dir)\n",
    "        except:\n",
    "            print(no + ' is not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse\n",
    "The function `oraccjsonparser()` takes one argument (the name of the `.json` file). It looks for the prefix `textid` to retrieve the six-digit P, Q, or X number of the text artifact. Parsing the file sequentially the code looks for the places where a line starts (`'.type' = 'line-start'`) and where a word starts (`'.node' = 'l'`, where `l` is for \"lemma\"). At each level the code will retrieve the relevant data and create a list of dictionaries where each dictionary represents a single word. \n",
    "\n",
    "Words not only include lemmatized words, but also unlemmatized and unlemmatizable words (such as breaks).\n",
    "\n",
    "The resulting dictionary includes keys such as `lang` (for language), `guideword`, `sense`, etc. - all the elements that define an [ORACC](http://oracc.org) signature. The dictionary also includes the keys `id_line` (a sequential number for each line or line-like element in a text) and `id_word` (a sequential number for each word in a line) that allow the user to reassemble a text in the original word and line order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def oraccjsonparser(file):\n",
    "    filename = '../data/json/corpusjson/' + file +'.json'\n",
    "    with open(filename, 'r') as d:\n",
    "        parser = ijson.parse(d)\n",
    "        word_l = []\n",
    "        word_d = {}\n",
    "        line_start = False\n",
    "        word_start = False\n",
    "        nonx = False\n",
    "        for prefix, event, value in parser: #prefix, event, and value are the standard\n",
    "                                            #elements of a JSON entry recognized by ijson\n",
    "            if prefix == 'textid':\n",
    "                id_text = value\n",
    "            if prefix.endswith('.type'):\n",
    "                if value == 'line-start':\n",
    "                    line_start = True\n",
    "                else:\n",
    "                    line_start = False\n",
    "            if line_start:\n",
    "                if prefix.endswith('.ref') and not word_start:\n",
    "                    id_line = value # id_line is a reference number for a line\n",
    "                                    # that includes the id_text (e.g. P123456.49)\n",
    "                if prefix.endswith('.label'):\n",
    "                    label = value   # label is a human-readable line number of the format\n",
    "                                    # o ii 24' (obverse column 2 line 24')\n",
    "            if prefix.endswith('node'):\n",
    "                if value == 'l':\n",
    "                    word_start = True\n",
    "                    if not word_d == {}:\n",
    "                        word_l.append(word_d) # append the previous word to the list\n",
    "                    word_d = {}               # and start a new dictionary\n",
    "                    word_d['id_text'] = id_text # provide each word with appropriate \n",
    "                    word_d['id_line'] = id_line # text and line-ID\n",
    "                    word_d['label'] = label     # and the line label.\n",
    "                else:\n",
    "                    word_start = False\n",
    "            if word_start:\n",
    "                if prefix.endswith('.ref'):\n",
    "                    word_d['id_word'] = value\n",
    "                if prefix.endswith('.sig'):\n",
    "                    word_d['signature'] = value\n",
    "                if '.f.' in prefix:\n",
    "                    category = re.sub('.*\\.', '', prefix) # get element after the last dot of the prefix\n",
    "                    word_d[category] = value # copy each element into the dictionary\n",
    "            if prefix.endswith('.type'):\n",
    "                if value == 'nonx':\n",
    "                    nonx = True\n",
    "                else:\n",
    "                    nonx = False\n",
    "            if nonx:                         # this captures so-called $-lines with information\n",
    "                if prefix.endswith('.ref'):  # about number of broken lines/columns.\n",
    "                    id_line = value          # $-lines have their own id_line.\n",
    "                if prefix.endswith('.strict'):\n",
    "                    if value == '1':           # select only 'strict' $ lines\n",
    "                        if not word_d == {}:\n",
    "                            word_l.append(word_d)\n",
    "                        word_d = {}\n",
    "                        word_d['id_line'] = id_line\n",
    "                        word_d['id_text'] = id_text\n",
    "                    else:\n",
    "                        nonx = False\n",
    "                if prefix.endswith('.extent'): # capture the three elements of strict $ lines\n",
    "                    word_d['extent'] = value   # namely extent, scope, and state.\n",
    "                if prefix.endswith('.scope'):\n",
    "                    word_d['scope'] = value\n",
    "                if prefix.endswith('.state'):\n",
    "                    word_d['state'] = value\n",
    "\n",
    "    word_l.append(word_d)  # make sure that the last word is captured, too.\n",
    "    return(word_l) # return a list of dictionaries, where each entry (dictionary) in\n",
    "                   # the list represents a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call the Parser Function for Each Textid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_l = []\n",
    "for no in tqdm.tqdm(pqxnos):\n",
    "    id_text = no[-7:]\n",
    "    try:\n",
    "        word_l.extend(oraccjsonparser(id_text))\n",
    "    except:\n",
    "        print(no + ' not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the Data into a DataFrame\n",
    "\n",
    "If a text has no breakage information in the form of `$ 1 line broken` (etc.) the fields `extent`, `scope`, and `state` do not exist. The fields `extent` and `scope` are referenced in the code below. After creating the dataframe the existence of these two fields is checked - if they do not exist, empty columns are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words = pd.DataFrame(word_l)\n",
    "if not 'extent' in words.columns:\n",
    "    words['extent'] = ''\n",
    "if not 'scope' in words.columns:\n",
    "    words['scope'] = ''\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Spaces and Commas from Guide Word and Sense\n",
    "Spaces in Guide Word and Sense may cause trouble in computational methods in tokenization, or when saved in Comma Separated Values format. All spaces and commas are replaced by hyphens or nothing, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = words.fillna('') # first replace Missing Values by empty string\n",
    "words['sense'] = [x.replace(' ', '-') for x in words['sense']]\n",
    "words['sense'] = [x.replace(',', '') for x in words['sense']]\n",
    "words['gw'] = [x.replace(' ', '-') for x in words['gw']]\n",
    "words['gw'] = [x.replace(',', '') for x in words['gw']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns in the resulting DataFrame correspond to the elements of a full [ORACC](http://oracc.org) signature, plus information about text, line, and word ids:\n",
    "* base (Sumerian only)\n",
    "* cf (Citation Form)\n",
    "* cont (continuation of the base; Sumerian only)\n",
    "* epos (Effective Part of Speech)\n",
    "* form (transliteration, omitting all flags such as indication of breakage)\n",
    "* gw (Guide Word: main or first translation in standard dictionary)\n",
    "* id_line (a line ID that begins with the six-digit P, Q, or X number of the text)\n",
    "* id_text (six-digit P, Q, or X number)\n",
    "* id_word (word ID that begins with the ID number of the line)\n",
    "* label (traditional line number in the form o ii 2' (obverse column 2 line 2'), etc.)\n",
    "* lang (language code, including sux, sux-x-emegir, sux-x-emesal, akk, akk-x-stdbab, etc)\n",
    "* morph (Morphology; Sumerian only)\n",
    "* norm (Normalization: Akkadian)\n",
    "* norm0 (Normalization: Sumerian)\n",
    "* pos (Part of Speech)\n",
    "* sense (contextual meaning)\n",
    "* signature (full ORACC signature)\n",
    "\n",
    "Not all data elements (columns) are available for all words. Sumerian words never have a `norm`, Akkadian words do not have `norm0`, `base`, `cont`, or `morph`. Most data elements are only present when the word is lemmatized; only `lang`, `form`, `pos`, `id_word`, `id_line`, and `id_text` should always be there. An unlemmatized word has `pos` 'X' (for unknown). Broken words have `pos` 'u' (for 'unlemmatizable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulate\n",
    "The columns may be manipulated with standard Pandas methods to create the desired output. By way of example, the following code will create a column `lemma` with the format **cf[gw]pos** (for instance **lugal[king]N**). For words that have no lemmatization, `lemma` equals `form`. Only Sumerian words are allowed (and thus `lang` can be omitted) and in addition to the column `lemma` the column `base` is preserved; words that have no lemmatization take `form` as their base. Words and bases are concatenated to lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove  non-Sumerian words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lang = ['sux', ''] # note that 'lang' is empty in entries that indicate damage\n",
    "words = words.loc[words['lang'].str[:3].isin(lang)].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Lemma Column and Adjust Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words['lemma'] = words['cf'] # first element of lemma is the citation form\n",
    "words['lemma'] = [words['lemma'][i] + '[' + words['gw'][i] \n",
    "                     + ']' + words['pos'][i] \n",
    "                     if not words['lemma'][i] == '' \n",
    "                     else words['form'][i] +'[NA]NA' for i in range(len(words))]\n",
    "words['lemma'] = [lemma if not lemma == '[NA]NA' else '' for lemma in words['lemma'] ]\n",
    "words['base'] = [words['base'][i] if not words['base'][i] == '' \n",
    "                 or words['label'][i] == '' else words['form'][i] \n",
    "                 for i in range(len(words))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = words.groupby([words['id_line'], words['label']]).agg({\n",
    "        'lemma': ' '.join,\n",
    "        'base': ' '.join,\n",
    "        'extent': ''.join, \n",
    "        'scope': ''.join\n",
    "    }).reset_index()\n",
    "df        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create fields `id_text` and `line`\n",
    "The field `id_text` is the P-number of the document and is equal to the first seven characters of `id_line`. The field `line` is an artificial line counter (integer) that is used to keep things in the right order. It derives from `id_line` by keeping eveything after the last dot (note that the regular expression `.+\\.` is greedy and will match the longest possible string ending in a dot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None # this prevents Pandas from issuing warnings about chained assignments\n",
    "df['id_text'] = df['id_line'].str[:7]\n",
    "df['line'] = [int(re.sub('.+\\.', '', line)) for line in df['id_line']] #create a line number for sorting\n",
    "df = df.sort_values(['id_text', 'line']).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save in CSV Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = '../data/raw/' + filename[:-4] + '.csv'\n",
    "print('saving ' + filename)\n",
    "with open(filename, 'w') as w:\n",
    "    df.to_csv(w, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
