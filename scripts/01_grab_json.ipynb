{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing ORACC Data from corpus.json\n",
    "by Niek Veldhuis\n",
    "UC Berkeley\n",
    "\n",
    "March-June 2017\n",
    "\n",
    "# TODO\n",
    "* check that COFs are treated properly\n",
    "* check that lines that continue into the next line (as in bilinguals) are captured completely. Such lines are indicated in the json by the the addition of 'l' (lower case L) to the reference (.ref).\n",
    "* add definition of fields to list in Introduction\n",
    "\n",
    "# Note\n",
    "Currently the code will fetch a large zip file from ORACC, download it, extract certain files from the zip file and parse those. The zip file contains all data that belong to an ORACC project or sub-project. Since one may run this notebook several times for collecting data from the same project, this may not be the best process (the download will take place every time). Move the download process to a separate notebook, preceding the current one.\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Purpose of the code is to download [ORACC](http://oracc.org) (Open Richly Annotated Cuneiform Corpus) JSON files that contain textual data and produce a `.csv` file in the directory `data/raw` with the relevant data for use in the phylogenetics project. The JSON files contain all the transliteration and lemmatization data of an ORACC project (metadata are made available in a separate `.json` file). For an introduction to the various ORACC JSON files see the [ORACC Open Data](http://oracc.org/doc/opendata) page.\n",
    "\n",
    "The resulting data file includes various elements of the ORACC data structure. The current code will output a file with the following fields: \n",
    "\n",
    "* id_line\n",
    "* label\n",
    "* lemma\n",
    "* base\n",
    "* extent\n",
    "* scope\n",
    "\n",
    "The fields `extent` and `scope` capture the number of missing lines or columns.\n",
    "\n",
    "The selection of fields may be adjusted with standard `Pandas` functions.\n",
    "\n",
    "## Notes\n",
    "The current version of the script works with the `requests` library.  \n",
    "\n",
    "This notebook is written for **Python 3.5** with **Pandas 0.19** and **requests 2.18.1**.\n",
    "\n",
    "The notebook was written for the [Digital Humanities Phylogenetics](https://github.com/ErinBecker/digital-humanities-phylogenetics) project with Erin Becker of [Data Carpentry](http://www.datacarpentry.org). The particular data selection and data manipulation performed in this notebook are inspired by the needs of that project (for instance, non-Sumerian words are filtered out). It should be fairly easy to adapt the notebook to the purposes of any other project that wishes to use [ORACC](http://oracc.org) data.\n",
    "\n",
    "## Licensing\n",
    "This notebook may be downloaded, used and adapted without any restrictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd   \n",
    "import requests\n",
    "import zipfile\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input List of Text IDs\n",
    "Identify a list of text IDs (P, Q, and X numbers) in the directory `data/text_ids`. The IDs are six-digit P, Q, or X numbers preceded by a project abbreviation in the format 'PROJECT/P######' or 'PROJECT/SUBPROJECT/Q######'. For example:\n",
    "* dcclt/P117395\n",
    "* etcsri/Q001203\n",
    "* rinap/rinap1/Q003421\n",
    "\n",
    "The list should be created with a flat text editor such as Textedit or Emacs (not in a word processor such as MS Word), and the filename should end in `.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = input('Filename: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textids = '../data/text_ids/' + filename\n",
    "with open(textids, 'r') as f:\n",
    "    pqxnos = f.readlines()\n",
    "pqxnos = [x.strip() for x in pqxnos]\n",
    "pqxnos[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse\n",
    "The function `parsejson()` takes one argument (the `textid`).  \n",
    "\n",
    "Words not only include lemmatized words, but also unlemmatized and unlemmatizable words (such as breaks).\n",
    "\n",
    "The resulting dictionary includes keys such as `lang` (for language), `guideword`, `sense`, etc. - all the elements that define an [ORACC](http://oracc.org) signature. The dictionary also includes the key `id_word` (a sequential number for each word in each line) that allows the user to reassemble a text in the original word and line order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parsejson(text, lemm_l = None):\n",
    "    label = ''\n",
    "    if lemm_l == None:\n",
    "        lemm_l = []\n",
    "    for dictionary in text[\"cdl\"]:\n",
    "        if \"cdl\" in dictionary: \n",
    "            parsejson(dictionary, lemm_l)\n",
    "        if \"label\" in dictionary:\n",
    "            label = dictionary[\"label\"]\n",
    "        if \"f\" in dictionary:\n",
    "            lemma = dictionary[\"f\"]\n",
    "            lemma[\"id_word\"] = dictionary[\"ref\"]\n",
    "            lemma[\"label\"] = label\n",
    "            lemm_l.append(lemma)\n",
    "        if \"strict\" in dictionary and dictionary[\"strict\"] == \"1\":\n",
    "            keys = [\"extent\", \"scope\",\"state\"]\n",
    "            lemma = {key: dictionary[key] for key in keys}\n",
    "            lemma[\"id_word\"] = dictionary[\"ref\"] + \".0\"\n",
    "            lemm_l.append(lemma)\n",
    "    return lemm_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call the Parser Function for Each Textid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_l = []\n",
    "for id_text in pqxnos:\n",
    "    project = id_text[:-8].lower()\n",
    "    pqx = id_text[-7:].upper()\n",
    "    url = \"http://oracc.org/\" + project + '/corpusjson/' + pqx + '.json'\n",
    "    r = requests.get(url)\n",
    "    try:\n",
    "        text = r.json()\n",
    "        print(\"parsing \" + id_text)\n",
    "        lemm_text = parsejson(text)\n",
    "        for lemm in lemm_text:\n",
    "            lemm['textid'] = id_text\n",
    "        word_l.extend(lemm_text)\n",
    "    except:\n",
    "        print(id_text + ' is not available or not complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the Data into a DataFrame\n",
    "\n",
    "If a text has no breakage information in the form of `$ 1 line broken` (etc.) the fields `extent`, `scope`, and `state` do not exist. The fields `extent` and `scope` are referenced in the code below. After creating the dataframe the existence of these two fields is checked - if they do not exist, empty columns are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words = pd.DataFrame(word_l)\n",
    "if not 'extent' in words.columns:\n",
    "    words['extent'] = ''\n",
    "if not 'scope' in words.columns:\n",
    "    words['scope'] = ''\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Spaces and Commas from Guide Word and Sense\n",
    "Spaces in Guide Word and Sense may cause trouble in computational methods in tokenization, or when saved in Comma Separated Values format. All spaces and commas are replaced by hyphens or nothing, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = words.fillna('') # first replace Missing Values by empty string\n",
    "words['sense'] = [x.replace(' ', '-') for x in words['sense']]\n",
    "words['sense'] = [x.replace(',', '') for x in words['sense']]\n",
    "words['gw'] = [x.replace(' ', '-') for x in words['gw']]\n",
    "words['gw'] = [x.replace(',', '') for x in words['gw']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns in the resulting DataFrame correspond to the elements of a full [ORACC](http://oracc.org) signature, plus information about text, line, and word ids:\n",
    "* base (Sumerian only)\n",
    "* cf (Citation Form)\n",
    "* cont (continuation of the base; Sumerian only)\n",
    "* epos (Effective Part of Speech)\n",
    "* form (transliteration, omitting all flags such as indication of breakage)\n",
    "* gw (Guide Word: main or first translation in standard dictionary)\n",
    "* id_line (a line ID that begins with the six-digit P, Q, or X number of the text)\n",
    "* id_text (six-digit P, Q, or X number)\n",
    "* id_word (word ID that begins with the ID number of the line)\n",
    "* label (traditional line number in the form o ii 2' (obverse column 2 line 2'), etc.)\n",
    "* lang (language code, including sux, sux-x-emegir, sux-x-emesal, akk, akk-x-stdbab, etc)\n",
    "* morph (Morphology; Sumerian only)\n",
    "* norm (Normalization: Akkadian)\n",
    "* norm0 (Normalization: Sumerian)\n",
    "* pos (Part of Speech)\n",
    "* sense (contextual meaning)\n",
    "* signature (full ORACC signature)\n",
    "\n",
    "Not all data elements (columns) are available for all words. Sumerian words never have a `norm`, Akkadian words do not have `norm0`, `base`, `cont`, or `morph`. Most data elements are only present when the word is lemmatized; only `lang`, `form`, `pos`, `id_word`, `id_line`, and `id_text` should always be there. An unlemmatized word has `pos` 'X' (for unknown). Broken words have `pos` 'u' (for 'unlemmatizable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulate\n",
    "The columns may be manipulated with standard Pandas methods to create the desired output. By way of example, the following code will create a column `lemma` with the format **cf[gw]pos** (for instance **lugal[king]N**). For words that have no lemmatization, `lemma` equals `form`. Only Sumerian words are allowed (and thus `lang` can be omitted) and in addition to the column `lemma` the column `base` is preserved; words that have no lemmatization take `form` as their base. Words and bases are concatenated to lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove  non-Sumerian words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lang = ['sux', ''] # note that 'lang' is empty in entries that indicate damage\n",
    "words = words.loc[words['lang'].str[:3].isin(lang)].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Lemma Column and Adjust Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words['lemma'] = words['cf'] # first element of lemma is the citation form\n",
    "words['lemma'] = [words['lemma'][i] + '[' + words['gw'][i] \n",
    "                     + ']' + words['pos'][i] \n",
    "                     if not words['lemma'][i] == '' \n",
    "                     else words['form'][i] +'[NA]NA' for i in range(len(words))]\n",
    "words['lemma'] = [lemma if not lemma == '[NA]NA' else '' for lemma in words['lemma'] ]\n",
    "words['base'] = [words['base'][i] if not words['base'][i] == '' \n",
    "                 or words['label'][i] == '' else words['form'][i] \n",
    "                 for i in range(len(words))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by Line\n",
    "Create a `line_id` from `word_id`. The field `word_id` has the format `Q000039.76.1` (first word in line 76 of Q000039). The corresponding `line_id` is `Q000039.76`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words[\"id_line\"] = [item[:item.rfind(\".\")] for item in words[\"id_word\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = words.groupby([words['id_line'], words['label']]).agg({\n",
    "        'lemma': ' '.join,\n",
    "        'base': ' '.join,\n",
    "        'extent': ''.join, \n",
    "        'scope': ''.join\n",
    "    }).reset_index()\n",
    "df        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create fields `id_text` and `line`\n",
    "The field `id_text` is the P-number of the document and is equal to the first seven characters of `id_line`. The field `line` is an artificial line counter (integer) that is used to keep things in the right order. It derives from `id_line` by keeping eveything after the last dot (note that the regular expression `.+\\.` is greedy and will match the longest possible string ending in a dot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None # this prevents Pandas from issuing warnings about chained assignments\n",
    "df['id_text'] = df['id_line'].str[:7]\n",
    "df['line'] = [int(line[line.rfind(\".\")+1:]) for line in df['id_line']] #create a line number for sorting\n",
    "df = df.sort_values(['id_text', 'line']).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save in CSV Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = '../data/raw/' + filename[:-4] + '.csv'\n",
    "print('saving ' + filename)\n",
    "with open(filename, 'w') as w:\n",
    "    df.to_csv(w, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
