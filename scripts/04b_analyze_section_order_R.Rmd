---
title: "04b_analyze_section_order"
author: "Erin Becker"
date: "November 3, 2017"
output: html_document
---

```{r setup, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_knit$set(root.dir = "~/Box Sync/digital-humanities-phylogenetics/")
```

# Section order

Here we analyze document similarity based on the degree of shared order of sections within a document. Sections are defined in 04_define_sections.Rmd. 

Our approach is based on the idea of *synteny* from the field of genomics. Two (or more) genes are said to have synteny if those genes are found in proximity across a set of genomes. The degree of synteny is proportional to the frequency with which genes are colocalized and the evolutionary distance of organisms in which that colocalization occurs.  

A genome-wide *synteny index* between a pair of genomes can be defined by calculating the *synteny* for all genes shared between that genome pair and taking the average.

[Reference](http://www.tandfonline.com/doi/full/10.1080/2159256X.2015.1120576)

For this analysis, **genes** are equivalent to **sections** (not **entries**) and **genomes** are equivalent to **documents**.  

### Limitations/Issues: 
- SI is only defined for sections shared between two documents. If there is no overlap in sections, the genome-wide SI is 0. Short documents will tend to have extremes of SI, either 1 (all sections shared) or 0 (no sections shared). We will include only documents above a certain section-number threshold for this analysis.

- A reasonable value of `k` (neighborhood size) will need to be determined by trial and error. This may also depend on the distribution of document lengths across our corpus. 

- This method was developed for microbial genomes, which are usually circular. Our documents are not circular, so we will need to decide how to treat sections that occur within `k` sections of the beginning or end of a document.
    - Idea: add a section called "top" and one called "bottom" when known to a document. Treat these as sections in the SI calculations.
    
- The document wide SI is the average SI for of all shared sections. Non-shared sections are not included in the calculation.  
- All of our documents are broken. We usually don't know whether a section is absent from a document because the original scribe did not include it or because it was originally included and subsequently lost due to damage. This also occurs in microbial genomes, particularly in metagenomic sequencing where incomplete genomes are assembled.
    
- How should we deal with sections that appear multiple times in a document? This happens in microbial genomes when genes are repeated within a genome. 
    
- When two sections are flipped in order, this method wouldn't notice (unless the documents are relatively long compared to k). Do we want it to?

### Implementation strategy
Our strategy for implementing this method is the following:

1) Determine the order of sections within each document in the corpus.  
2) For each document pair:
    - Define the intersection of sections present in that pair.
    - For each section in the intersection, count the number of shared sections in the k-neighborhood between the two documents.
3) Calculate document wide SI = average SI for all sections present in both documents.
4) Calculate distance = 1 - document wide SI for any document pair
5) Calculate a tree from the distance matrix.
6) Bootstrap.
7) Calculate consensus tree.

```{r}
# install.packages(c("ggplot2", "ggdendro", "reshape", "scales", "RColorBrewer", "gtools", "reshape2"))

#source("http://bioconductor.org/biocLite.R")
#biocLite("BiocInstaller")
#biocLite("phangorn")

library(ggplot2)
library(ggdendro)
library(reshape)
library(reshape2)
library(scales)
library(RColorBrewer)
library(gtools)
library(phangorn)
```

## Read in section definitions
Sections were defined [previously](https://github.com/ErinBecker/digital-humanities-phylogenetics/blob/master/scripts/04_define_sections.Rmd) and are read in from file here.

```{r}
Q39_sections = read.csv("data/sections/Q39_sections.csv", stringsAsFactors = FALSE)
```

We remove sections that have no citation forms (these sections have column names
that begin with "X.") and are shown below.

```{r eval = TRUE}
# preview X. sections (possible garbage sections)
head(Q39_sections[,grep("X\\.", colnames(Q39_sections))])
```

```{r echo = TRUE}
Q39_sections = Q39_sections[,-grep("X\\.", colnames(Q39_sections))]
```

## Read in a corpus to analyze
We now read in a csv file containing the data from all of our exemplar texts (and a few unrelated documents to act as outgroups).

```{r}
Q39_corpus = read.csv("data/pass/Q39_par.csv", stringsAsFactors = FALSE)
```

There are `r length(unique(corpus$id_text))` documents in our corpus, including
our composite text.

### Step 1: Determine order of sections for each document in corpus
For each entry in each document in the corpus, identify which section it belongs to. Add this section identification to a new column in the dataframe. If an entry is found in more than one section, section names are separated by ":".

```{r echo = TRUE}
add_sections_to_entries = function(df, section_defs) {
  
  for (i in 1:nrow(df)) {
      entry = tolower(df$entry[i])
      sections = names(which(sapply(section_defs, function(x) any(x == entry)) == TRUE))
      if (length(sections) == 0) df$section[i] = NA
      else df$section[i] = paste(sections, collapse = ":")
  }
  df
}

Q39_corpus = add_sections_to_entries(Q39_corpus, Q39_sections)
```

We can preview the section definitions:

```{r}
head(Q39_corpus[,c("id_text", "entry", "section")], n = 10)
```

Many entries will not be assigned to a section because they either do not appear verbatium in the Q text that was used to define sections or they do appear in the Q text, but were not part of a section (i.e. they were not adjacent to another entry that shared enough similarity to be considered part of the same section).

We then check how common it was for entries to be assigned to more than one section. 

```{r echo = TRUE}
unique(Q39_corpus[grep(":", Q39_corpus$section),]$entry)
```

Only the entry "ŋešgana[pestle]N" appears in two sections in the Q39 corpus. This entry appears in several documents. 

```{r echo = TRUE}
unique(Q39_corpus[grep(":", Q39_corpus$section),])
```

Now that we know which section(s) each entry belongs to, we can define the order
of sections for each document. First, we split our dataframe by id_text so that
each document is in its own dataframe. We then have a list of dataframes. We then
extract the "section" column from each of those dataframes. This gives us a list
of character vectors.

```{r}
Q39_corpus$id_text = as.factor(Q39_corpus$id_text)
Q39_docs = split(Q39_corpus, Q39_corpus$id_text)
Q39_docs = sapply(Q39_docs, function(x) x$section)
head(Q39_docs)
```

We need to get rid of `NA`s. These represent entries that were not assigned to a section. Then we retain only documents that are not of length zero (i.e. documents that had at least one entry assigned to a section).

```{r}
Q39_docs_no_nas = sapply(Q39_docs, function(x) x[!is.na(x)])

Q39_non_empty_docs = sapply(Q39_docs_no_nas, function(x) length(x) > 0)

Q39_docs_no_nas = Q39_docs_no_nas[Q39_non_empty_docs]
head(Q39_docs_no_nas)
```

This leaves us with `r length(docs_no_nas)` documents in our corpus.

We can then use the `rle()` function ("run length encoding") to get a character
vector of the sections in each document, in order. This function calculates the
lengths and values of runs in a sequence. We're not interested in the lengths of
the runs (for this analysis), but can extract the values (names of sections). We end up with a list of character vectors.

```{r}
Q39_doc_sections = lapply(Q39_docs_no_nas, 
                      function(x) unname(rle(unlist(x))$values))
                      
head(Q39_doc_sections)
```

We will focus our analysis on documents with eight or more sections.

```{r echo = TRUE}
Q39_num_sections = sapply(Q39_doc_sections, length)
hist(Q39_num_sections, breaks = 20)
Q39_doc_sections_keep = Q39_doc_sections[which(Q39_num_sections >= 8)]
```

There are `r length(doc_sections_keep)` documents remaining in our corpus.

### Method 1: Repeated sections combined

One important consideration in calculating section order is in determining how
to count sections that appear multiple times within a document. For this method,
we consider repeated sections as a single section and combine their k-neighborhoods.

We first define function to get the section neighborhood for any given section in
any given document. This function must be agnostic to the number of times a
section appears in a document, combining the neighborhoods for all times that
section appears. Individual neighborhoods are **neighborhood**, combined
neighborhoods are **n**. 

Note also that the complicated indexing within the for loop is required to avoid
going out of bounds. This is important for the first few and last few sections 
in a document. Going out of bounds into negative numbers will throw a `Traceback
error`, however, going out of bounds into numbers larger than the length of the
vector you're indexing will add `NA`s to your resulting vector. This is worse
because the `NA` is counted as an item in the intersection!

Another thing to notice is that the grep statement only finds whole word matches.
This is accomplished by using the "beginning of string" `^` and "end of string"
`$` metacharacters within the regular expression search.

```{r echo = TRUE}
get_neighborhood = function(doc, i, k) {
    
    # create vector to store combined neighborhoods
    n = c()
    
    # search for a section within a document and extract neighborhood
    for(x in grep(paste0("^",i,"$"), doc)) {
    
        if(x >= k+1 & x <= length(doc) - k) sequence = (x - k):(x + k)
        else if(x < k+1) sequence = 1:(x + k)
        else if(x > length(doc) - k) sequence = (x - k):length(doc)
    
        neighborhood = doc[sequence]
    
        # add neighborhood to combined neighborhoods for repeated sections
        n = c(n, neighborhood)
        #print(paste("The combined neighborhood is:", n))
}
        # return combined neighborhood    
        n 
}
```

    
Next we define a function to calculate a document-wide synteny index (SI) for any given document pair. 

```{r}
calc_SI = function(A, B, k) {

    I = intersect(A, B)
    counter = 0

    for (i in I) {
        n_A = get_neighborhood(A, i, k)
        n_B = get_neighborhood(B, i, k)
        SI = length(na.omit(intersect(n_A, n_B))) - 1 # subtract the shared section iteself

        counter = counter + SI
        }

return(c(SI = counter/length(I), num_shared = length(I), k = k))
    }
```

Next we create a dataframe to store our results. It will have the following columns: 

* `doc_A` = text_id for first document in compared pair
* `doc_B` = text_id for second document in compared pair
* `num_shared_sec` = length(I) or number of shared sections between the two documents
* `SI` = document wide SI

The function `combn` allows us to get all possible combinations of documents. `t` transposes the resulting data.frame.

```{r}
Q39_docs = names(Q39_doc_sections_keep)

make_df = function(docs) {
  all_pairs = permutations(length(docs), 2, docs, repeats.allowed = T)
  all_pairs = as.data.frame(all_pairs, stringsAsFactors = FALSE)

  df = data.frame("doc_A" = all_pairs[,1], "doc_B" = all_pairs[,2],
                        "num_shared_sec" = numeric(nrow(all_pairs)), 
                       "SI" = numeric(nrow(all_pairs)), 
                          "k" = numeric(nrow(all_pairs)))
  df$doc_A = as.character(df$doc_A)
  df$doc_B = as.character(df$doc_B)
  df
}

Q39_all_pairs_SI = make_df(Q39_docs)
```

The `all_pairs` object includes self-self pairs (A=B) and duplicate pairs with reversed order (A-B and B-A). (EXTRA)

Now we use the `calc_SI` function to iterate through each of the document pairs and store the results in our data.frame `all_pairs_SI`.

```{r}
fill_df = function(df, doc_sections_keep) {
  for (i in 1:nrow(df)) {
    A = df[i,]$doc_A
    B = df[i,]$doc_B
    result = calc_SI(doc_sections_keep[[A]], doc_sections_keep[[B]], k = 2)
    df[i,]$SI = result["SI"]
    df[i,]$num_shared_sec = result["num_shared"]
    df[i,]$k = result["k"] 
  } 
  df }

Q39_all_pairs_SI = fill_df(Q39_all_pairs_SI, Q39_doc_sections_keep)
```

```{r}
summary(Q39_all_pairs_SI$SI)
```

# Complications in calculating maximum document-wide SI

The maximum possible SI for a pair of document should be 2*k. However, there are a few complications: 

1) Our documents are not circular. Therefore, the first k and last k sections in a document can not have 2*k neighbors. This reduces the maximum possible SI for a given document pair and makes the actual maximum SI possible dependent on document length. 

2) Because some sections are repeated more than once in a document, and this method pools the neighborhoods for all times a section appears in a document, the maximum calculated SI can be greater than 2*k. In fact, the maximum calculated SI can be arbitraribly large, if many sections are repeated within a document.

For this dataset, all instances where SI is > 2k are for self-self pairs. 

```{r}
# Visually inspect that all cases of SI > 2k are self-self.
k = unique(Q39_all_pairs_SI$k)
Q39_all_pairs_SI[which(Q39_all_pairs_SI$SI > 2*k),]
```

We can set a "ceiling" SI to 4. 

```{r echo = TRUE}
# change all SI > 2k to 2k
Q39_all_pairs_SI[which(Q39_all_pairs_SI$SI > 2*k),]$SI = 2*k
```

We also have many document pairs with an SI of NaN. These appear to be cases where two documents have no shared sections (and therefore SI was calculated as 0/0). The following checks that this is the case for all instances of SI = NaN. 

```{r echo = TRUE}
# check that all doc pairs with SI of NaN have zero shared sections
unique(Q39_all_pairs_SI[which(is.na(Q39_all_pairs_SI$SI)),]$num_shared_sec)
```

Set the SI for these document pairs to be -2k. 

```{r echo = TRUE}
# if the condition above is true, change SI of NaN to -2k
# so that distance will be greatest for these doc pairs
Q39_NA_rows = which(is.na(Q39_all_pairs_SI$SI) == TRUE)
Q39_all_pairs_SI[Q39_NA_rows,]$SI = -2*k
```

We can now calculate the distance between any document pair and add that to
our dataframe. 

```{r}
# Calculate distance and add to dataframe
Q39_all_pairs_SI$dist = (2*Q39_all_pairs_SI$k)-(Q39_all_pairs_SI$SI)
head(Q39_all_pairs_SI)
```

And visualize the distribution of distances as a histogram. 

```{r}
hist(Q39_all_pairs_SI$dist, breaks = 30)
```

Next we extract the columns of our data frame that we need
to build a distance tree.

```{r}
# Extract the columns of the data frame we need to build a dendrogram
Q39_all_pairs_SI = Q39_all_pairs_SI[,c("doc_A", "doc_B", "dist")]
head(Q39_all_pairs_SI)
```

We will round the distances to two decimal places.

```{r}
# round the distances
Q39_all_pairs_SI$dist = round(Q39_all_pairs_SI$dist, 2)
head(Q39_all_pairs_SI)
```

### Create bootstrapped corpuses

We now create many different versions of the input corpus by resampling from within the corpus file (QXX_par). This will enable us to evaluate the quality of each branching point on our dendrograms
based on the fraction of dendrograms generated from these bootstrapped corpuses that 
replicate that branching point. Note that eval is set to FALSE so that these don't regenerate every time the file is knit.

```{r echo = TRUE, eval = FALSE}

Q39_par = read.csv("~/Box Sync/digital-humanities-phylogenetics/data/pass/Q39_par.csv", stringsAsFactors = FALSE)

for(i in 1:1000) {
  filename = paste0("~/Box Sync/digital-humanities-phylogenetics/data/pass/bootstrap/Q39_par_bootstrap/Q39_par_bootstrap_", i, ".csv")
  rows = sample(1:nrow(Q39_par), replace = TRUE)
  corpus_bootstrap = Q39_par[rows,]
  write.csv(corpus_bootstrap, filename, row.names = FALSE)
}
```

### Run through bootstrapped corpus files and build distance tree for each

Define a function to carry out the whole workflow:

```{r echo = TRUE}
calculate_SI = function(sections_file, corpus_file, k, dist_file) {
  # read in section definitions
  sections = read.csv(sections_file, stringsAsFactors = FALSE)
  
  # get rid of X. sections
  sections = sections[,-grep("X\\.", colnames(sections))]
  
  # read in corpus
  corpus = read.csv(corpus_file, stringsAsFactors = FALSE)
  
  # sort corpus by document id and line number
  # so that bootstrapped corpuses are in correct order
  corpus = corpus[order(corpus$id_text, corpus$id_line),]
  
  # add sections to entries
  corpus = add_sections_to_entries(corpus, sections)
  
  # define order of sections within each document
  corpus$id_text = as.factor(corpus$id_text)
  docs = split(corpus, corpus$id_text)
  docs = sapply(docs, function(x) x$section)
  
  # get rid of entries that weren't assigned to a section
  docs_no_nas = sapply(docs, function(x) x[!is.na(x)])
  non_empty_docs = sapply(docs_no_nas, function(x) length(x) > 0)
  docs_no_nas = docs_no_nas[non_empty_docs]
  
  # extract a character vector of the sections in each document, in order
  doc_sections = lapply(docs_no_nas, 
                      function(x) unname(rle(unlist(x))$values))
  
  # include only documents with 8 or more sections
  num_sections = sapply(doc_sections, length)
  doc_sections_keep = doc_sections[which(num_sections >= 8)]
  doc_names = names(doc_sections_keep)
  
  # calculate the document-wide SI for each document pair
  all_SI = make_df(doc_names)
  all_SI = fill_df(all_SI, doc_sections_keep)

  # change all SI > 2k to 2k
  all_SI[which(all_SI$SI > 2*k),]$SI = 2*k

  # set SI for document pairs with no shared sections to -2k
  NA_rows = which(is.na(all_SI$SI) == TRUE)
  all_SI[NA_rows,]$SI = -2*k
  
  # Calculate distance and add to dataframe
  all_SI$dist = (2*all_SI$k) - (all_SI$SI)

  # Extract the columns of the data frame we need to build a dendrogram
  all_SI = all_SI[,c("doc_A", "doc_B", "dist")]
  
  write.csv(all_SI, file = dist_file)
}
```

Run the workflow above for all of the bootstrapped corpuses and write out a new set of output files containing the calculated distance matrix for each corpus.

```{r echo = TRUE, eval = FALSE}

for(i in 1:10) {
  filename_in = paste0("data/", i, ".csv")
  filename_out = paste0("data/", i, ".csv")
  corpus = read.csv(filename_in)
  all_pairs_SI = calculate_SI(corpus)
  write.csv(all_pairs_SI, filename_out)
}
```

Compute a neighbor joining tree for each of the calculated distance matrices and write out the tree file.

```{r echo = TRUE, eval = FALSE}
# Build neighbor joining tree for each distance matrix
for(i in 21:1000) {
  filename_in = paste0("data/dtm_data/bootstrap/distance_matrices/all_pairs_", i, ".csv")
  filename_out = paste0("data/dtm_data/bootstrap/trees/dtm_nj_", i, ".tre")
  all_pairs = read.csv(filename_in, row.names = 1)
  all_pairs_matrix = data.matrix(acast(all_pairs, doc1 ~ doc2, 
                                       value.var='distance', fun.aggregate = sum, margins = FALSE))
  treeNJ  <- NJ(dist(all_pairs_matrix))
  write.tree(treeNJ, filename_out)
}
```

Compute a consensus tree for all generated tree files.

```{r echo = TRUE, eval = FALSE}

trees = list()

for(i in 1:1000) {
  filename_in = paste0("data/", i, ".tre")
  name = paste0("tree_", i)
  tree = read.tree(filename_in)
  trees[[name]] <- tree
  }

consensus_50 = consensus(trees, p = 0.5)
write.tree(consensus_50, "....bootstrap/consensus_trees/consensus_50.tre")

consensus_75 = consensus(trees, p = 0.75)
write.tree(consensus_75, "..../bootstrap/consensus_trees/consensus_75.tre")
```

