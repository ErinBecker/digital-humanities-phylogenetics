---
title: "04b_analyze_section_order"
author: "Erin Becker"
date: "November 3, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_knit$set(root.dir = "~/Box Sync/digital-humanities-phylogenetics/")
```

# Section order

Here we analyze document similarity based on the degree of shared order of sections within a document. Sections are defined in 04_define_sections.Rmd. 

Our approach is based on the idea of *synteny* from the field of genomics. Two (or more) genes are said to have synteny if those genes are found in proximity across a set of genomes. The degree of synteny is proportional to the frequency with which genes are colocalized and the evolutionary distance of organisms in which that colocalization occurs.  

A genome wide *synteny index* between a pair of genomes can be defined by calculating the *synteny* for all genes shared between that genome pair.

[Reference](http://www.tandfonline.com/doi/full/10.1080/2159256X.2015.1120576)

For this analysis, **genes** are equivalent to **sections** (not **entries**) and **genomes** are equivalent to **documents**.  

### Limitations/Issues: 
- SI is only defined for sections shared between two documents. If there is no overlap in sections, the genome-wide SI is 0. Short documents will tend to have extremes of SI, either 1 (all sections shared) or 0 (no sections shared). We will include only documents above a certain section-number threshold for this analysis.

- A reasonable value of `k` will need to be determined by trial and error. This may also depend on the distribution of document lengths across our corpus. 

- This method was developed for microbial genomes, which are usually circular. Our documents are not circular, so we will need to decide how to treat sections that occur within `k` sections of the beginning or end of a document.
    - Idea: add a section called "top" and one called "bottom" when known to a document. Treat these as sections in the SI calculations.
    
- The document wide SI is the average SI for of all shared sections. Non-shared sections are not included in the calculation.  
    
- All of our documents are broken. We usually don't know whether a section is absent from a document because the original scribe did not include it or because it was originally included and subsequently lost from damage. was originally present in the non-broken document. This also occurs in microbial genomes, particularly in metagenomic sequencing where incomplete genomes are assembled.
    
- How should we deal with sections that appear multiple times in a document? This happens in microbial genomes when genes are repeated within a genome. 
    
- When two sections are flipped in order, this method wouldn't notice (unless the documents are relatively long compared to k). Do we want it to?

### Implementation strategy
Our strategy for implementing this method is the following:

1) Determine the order of sections within each document in corpus.  
2) For each document pair:
    - Define the intersection of sections present in that pair.
    - For each section in the intersection, count the number of shared sections in the k-neighborhood between the two documents.
3) Calculate document wide SI = average SI for all sections present in both documents.
4) Calculate distance = 1 - document wide SI for any document pair
5) Calculate a tree from the distance matrix.
6) Bootstrap.
7) Calculate consensus tree.

```{r}
# install.packages(c("ggplot2", "ggdendro", "reshape", "scales", "RColorBrewer", "gtools", "reshape2"))

#source("http://bioconductor.org/biocLite.R")
#biocLite("BiocInstaller")
#biocLite("phangorn")

library(ggplot2)
library(ggdendro)
library(reshape)
library(reshape2)
library(scales)
library(RColorBrewer)
library(gtools)
library(phangorn)
```

## Read in section definitions
Sections were defined [previously](https://github.com/ErinBecker/digital-humanities-phylogenetics/blob/master/scripts/04_define_sections.Rmd) and are read in from file here.

```{r}
Q1_sections = read.csv("data/sections/Q01_sections.csv", stringsAsFactors = FALSE)
Q39_sections = read.csv("data/sections/Q39_sections.csv", stringsAsFactors = FALSE)
Q40_sections = read.csv("data/sections/Q40_sections.csv", stringsAsFactors = FALSE)
Q41_sections = read.csv("data/sections/Q41_sections.csv", stringsAsFactors = FALSE)
Q42_sections = read.csv("data/sections/Q42_sections.csv", stringsAsFactors = FALSE)
```

```{r}
# preview X sections
head(Q39_sections[,grep("X.", colnames(Q39_sections))])
```

## Read in a corpus to analyze
df = read.csv("../pass/Q39_par.csv", stringsAsFactors = FALSE)
tail(df)
length(unique(df$id_text))

### Step 1: Determine order of sections for each document in corpus
For each entry in each document in the corpus, identify which section it belongs to. Add this section identification to a new column in the dataframe. If an entry is found in more than one section, section names are separated by ":".

section_defs = Q39_sections
for (i in 1:nrow(df)) {
    entry = tolower(df$entry[i])
    sections = names(which(sapply(section_defs, function(x) any(x == entry)) == TRUE))
    if (length(sections) == 0) df$section[i] = NA
    else df$section[i] = paste(sections, collapse = ":")
}
df

Many entries will not be assigned to a section because they either do not appear verbatium in the Q text that was used to define sections or they do appear in the Q text, but were not part of a section (i.e. they were not adjacent to another entry that shared enough similarity to be considered part of the same section).

Note to Erin: Try going back to the section definition notebook and adding entries in the Q text that don't join a section to the previous section.

Very few entries were assigned to more than one section. Only the entry "ŋešgana[pestle]N" appears in two sections in the Q39 corpus. This entry appears in several documents.

unique(df[grep(":", df$section),]$entry)

Now that we know which section(s) each entry belongs to, we can define the order of sections for each document. 

First, we split our dataframe by id_text so that each document is in its own dataframe. We then have a list of dataframes.

We then extract the "section" column from each of those dataframes. This gives us a list of character vectors.

df$id_text = as.factor(df$id_text)
docs = split(df, df$id_text)
docs = sapply(docs, function(x) x$section)
head(docs)

We need to get rid of `NA`s. These represent entries that were not assigned to a section. Then we retain only documents that are not of length zero (i.e. documents that had at least one entry assigned to a section).

docs_no_nas = sapply(docs, function(x) x[!is.na(x)])

non_empty_docs = sapply(docs_no_nas, function(x) length(x) > 0)

docs_no_nas = docs_no_nas[non_empty_docs]
head(docs_no_nas)

print(paste("This leaves us with", length(docs_no_nas), "documents."))

We can then use the `rle()` function ("run length encoding") to get a character vector of the sections in each document, in order. This function calculates the lengths and values of runs in a sequence. We're not interested in the lengths of the runs (for this analysis), but can extract the values (names of sections).

We end up with a list of character vectors.

doc_sections = lapply(docs_no_nas, 
                      function(x) unname(rle(unlist(x))$values))
                      
head(doc_sections)
tail(doc_sections)

We now want to get rid of all documents with only one section, as this will not be informative when comparing section order.

num_sections = sapply(doc_sections, length)
doc_sections_keep = doc_sections[-which(num_sections == 1)]
head(doc_sections_keep)

print(paste("This leaves us with", length(doc_sections_keep), "documents."))

print(paste(length(which(num_sections >= 20)), "documents have 20 sections or more.",
            length(which(num_sections >= 10)), "documents have 10 sections or more.", 
            "Those sections are:"))
print(names(which(num_sections >= 10)))

hist(num_sections, breaks = 20)

### Method 1: Repeated sections combined

This method considers repeated sections in a document as a single section and combines their k-neighborhoods.

First, we need a function to get the section neighborhood for any given section in any given document. The function below accomplishes this. Note that it is agnostic to the number of times a section appears in a document, combining the neighborhoods for all times that section appears. Individual neighborhoods are "neighborhood", combined neighborhoods are "n". 

Note also that the complicated indexing within the for loop is required to avoid going out of bounds. This is important for first few and last few sections in a document. Going out of bounds into negative numbers will throw a `Traceback error`, however, going out of bounds into numbers larger than the length of the vector you're indexing will add `NA`s to your resulting vector. This is worse because the `NA`s are counted in the intersection!

Another thing to notice is that the grep statement only finds whole word matches. This is accomplished by using the "beginning of string" `^` and "end of string" `$` metacharacters within the regular expression search.

get_neighborhood = function(doc, i, k) {
    
    # create vector to store combined neighborhoods
    n = c()
    
    # search for a section within a document and extract neighborhood
    for(x in grep(paste0("^",i,"$"), doc)) {
    
        if(x >= k+1 & x <= length(doc) - k) sequence = (x - k):(x + k)
        else if(x < k+1) sequence = 1:(x + k)
        else if(x > length(doc) - k) sequence = (x - k):length(doc)
    
        neighborhood = doc[sequence]
    
        # add neighborhood to combined neighborhoods for repeated sections
        n = c(n, neighborhood)
        #print(paste("The combined neighborhood is:", n))
}
        # return combined neighborhood    
        n 
    }
    
Our next step is to create a function to calculate a document-wide synteny index (SI) for any given document pair. 

A = doc_sections[["P370399"]]
B = doc_sections[["P370399"]]

print(A)
print(B)

calc_SI = function(A, B, k) {

    I = intersect(A, B)
    print(I)
    counter = 0

    for(i in I) {
        print(paste("Next section:", i))
        n_A = get_neighborhood(A, i, k)
        n_B = get_neighborhood(B, i, k)
        SI = length(na.omit(intersect(n_A, n_B))) - 1 # subtract the shared section iteself
        print(paste("The SI for", i, "is", SI))
        counter = counter + SI
        }

return(c(SI = counter/length(I), num_shared = length(I), k = k))
#counter/length(I)
    }

test = calc_SI(A, B, k = 2)
test

Now we will create a dataframe to store our results. It will have column `doc_A` = text_id for first document in compared pair, column `doc_B` = text_id for second document in compared pair, `num_shared_sec` = length(I) or number of shared sections between the two documents, and `SI` = document wide SI. 

The function `combn` allows us to get all possible combinations of documents. `t` transposes the resulting data.frame.

The theoretical maximum SI for a document pair depends on k, but is always less than 2k, because sections at the end and beginning of documents can never have a SI of 2k, even if they match perfectly with the comparison document. 

For k = 2, the maximum SI for a document pair is 3.66666666666667.


docs = names(doc_sections_keep)
#all_pairs = t(as.data.frame(combn(docs, 2)))
all_pairs = permutations(length(docs), 2, docs, repeats.allowed = T)
all_pairs = as.data.frame(all_pairs, stringsAsFactors = FALSE)

all_pairs_SI = data.frame("doc_A" = all_pairs[,1], "doc_B" = all_pairs[,2], 
                        "num_shared_sec" = numeric(length(all_pairs)), 
                       "SI" = numeric(length(all_pairs)), 
                          "k" = numeric(length(all_pairs)))
all_pairs_SI$doc_A = as.character(all_pairs_SI$doc_A)
all_pairs_SI$doc_B = as.character(all_pairs_SI$doc_B)

str(all_pairs_SI)

# check for self-self pairs
length(which(all_pairs$V1 == all_pairs$V2))
# check for duplicate pairs (A-B and B-A)
all_pairs[which(all_pairs$V1 == "P225033" & all_pairs$V2 == "P225086"),]
all_pairs[which(all_pairs$V1 == "P225086" & all_pairs$V2 == "P225033"),]

The `all_pairs` object includes self-self pairs (A=B) and duplicate pairs with reversed order (A-B and B-A). 

Now we use the `calc_SI` function to iterate through each of the document pairs and store the results in our data.frame `all_pairs_SI`.

for(i in 1:nrow(all_pairs_SI)) {
    A = all_pairs_SI[i,]$doc_A
    B = all_pairs_SI[i,]$doc_B
    result = calc_SI(doc_sections_keep[[A]], doc_sections_keep[[B]], k = 2)
    all_pairs_SI[i,]$SI = result["SI"]
    all_pairs_SI[i,]$num_shared_sec = result["num_shared"]
    all_pairs_SI[i,]$k = result["k"] 
}

summary(all_pairs_SI$SI)
nrow(all_pairs_SI)
head(all_pairs_SI)

The maximum possible SI for a pair of document should be 2*k. However, there are a few complications. 

1) Our documents are not circular. Therefore, the first k and last k sections in a document can not have 2*k neighbors. This reduces the maximum possible SI for a given document pair and makes the actual maximum SI possible dependent on document length. 

2) Because some sections are repeated more than once in a document (particularly "garbage" sections like X.6), and this method pools the neighborhoods for all times a section appears in a document, the maximum calculated SI can be greater than 2*k. In fact, the maximum calculated SI can be arbitraribly large, if many sections are repeated within a document.

For this dataset, all instances where SI is > 2k are for self-self pairs. However, that may not be the case for all datasets, so we should add a check here. For our dataset, however, we can set a "ceiling" SI to 4. 

# Visually inspect that all cases of SI > 2k are identical docs.
k = unique(all_pairs_SI$k)
all_pairs_SI[which(all_pairs_SI$SI > 2*k),]

# change all SI > 2k to 2k
all_pairs_SI[which(all_pairs_SI$SI > 2*k),]$SI = 2*k

We also have many document pairs with an SI of NaN. These appear to be cases where two documents have no shared sections (and therefore SI was calculated as 0/0). The following checks that this is the case for all instances of SI = NaN and sets the SI for these document pairs to be -2k. 

# check that all doc pairs with SI of NaN have zero shared sections
unique(all_pairs_SI[which(is.na(all_pairs_SI$SI)),]$num_shared_sec)

# if this condition is true provisionally replace SI of NaN to -2k
# so that distance will be greatest for these doc pairs
NA_rows = which(is.na(all_pairs_SI$SI) == TRUE)
all_pairs_SI[NA_rows,]$SI = -2*k
head(all_pairs_SI)

# Calculate distance and add to dataframe
all_pairs_SI$dist = (2*all_pairs_SI$k)-(all_pairs_SI$SI)
head(all_pairs_SI)

hist(all_pairs_SI$dist, breaks = 30)

# Extract the columns of the data frame we need to build a dendrogram
all_pairs_SI = all_pairs_SI[,c("doc_A", "doc_B", "dist")]
head(all_pairs_SI)

# round the distances
all_pairs_SI$dist = round(all_pairs_SI$dist, 2)

### Create bootstrapped corpuses

We now create many different versions of the input corpus by resampling from within the corpus file (QXX_par). This will enable us to evaluate the quality of each branching point on our dendrograms
based on the fraction of dendrograms generated from these bootstrapped corpuses that 
replicate that branching point. Note that eval is set to FALSE so that these don't regenerate every time the file is knit.

#### Note to self: 
Need to make sure the boostrapped corpuses are sorted before analyzing whether sequential entries are in the same section (or check to see if the algorithm checks for row number).

```{r echo = TRUE, eval = FALSE}

Q39_par = read.csv("~/Box Sync/digital-humanities-phylogenetics/data/pass/Q39_par.csv", stringsAsFactors = FALSE)

for(i in 1:1000) {
  filename = paste0("~/Box Sync/digital-humanities-phylogenetics/data/pass/bootstrap/Q39_par_bootstrap/Q39_par_bootstrap_", i, ".csv")
  rows = sample(1:nrow(Q39_par), replace = TRUE)
  corpus_bootstrap = Q39_par[rows,]
  write.csv(corpus_bootstrap, filename, row.names = FALSE)
}
```
