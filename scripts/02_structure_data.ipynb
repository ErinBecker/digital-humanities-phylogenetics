{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to dataset and data structure\n",
    "\n",
    "This notebook uses data from the Digital Corpus of Cuneiform Lexical Texts ([DCCLT](http://oracc.org/dcclt)) derived from parsed JSON files. For the JSON output from the Open Richly Annotated Cuneiform Corpus ([ORACC](http://oracc.org)) see the [ORACC Open Data documentation](http://oracc.museum.upenn.edu/doc/opendata/index.html).  \n",
    "\n",
    "The JSON files are parsed with the notebook [01_grab_json.ipynb](https://github.com/ErinBecker/digital-humanities-phylogenetics/blob/master/scripts/01_grab_json.ipynb). Output from that notebook is saved in the `../data/raw` directory.\n",
    "\n",
    "This notebook takes an input file from the `../data/raw` directory, identifying the text IDs of the documents to be parsed. The input file for our analyses is [ob_lists_wood_w_id_text.txt](https://github.com/ErinBecker/digital-humanities-phylogenetics/blob/master/data/raw/ob_lists_wood_w_id_text.csv), but we provide other texts in `../data/raw` to use for compartive analyses.\n",
    "\n",
    "The input file lists all the Text IDs of Old Babylonian lists of trees and wooden objects currently in [DCCLT](http://oracc.org/dcclt), as well as the composite text of the [Nippur version](http://oracc.org/Q000039). Text IDs consist of a P plus a six-digit number (commonly referred to as P-number) that is recognized by [ORACC](http://oracc.org) and by the Cuneiform Digital Library Initiative ([CDLI](http://cdli.ucla.edu)) and that has become the de-facto standard in Assyriology. [CDLI](http://cdli.ucla.edu) provides metadata (provenience, period, publication, museum number, etc) for each text. Composite text IDs consist of a Q plus a six-digit number (for instance Q000039). Texts that have not (yet) been cataloged in [CDLI](http://cdli.ucla.edu) receive a (temporary) six-digit X number.\n",
    "\n",
    "The output from this notebook is placed in the directory [data/pass](https://github.com/ErinBecker/digital-humanities-phylogenetics/tree/master/data/pass). The output is a comma-separated file with the following fields: \n",
    "\n",
    "| field         | description                     |\n",
    "|-----------\t|------------------------------------------------------------------------------------------------------------------------------------------------------\t|\n",
    "| id_line   \t| consists of a text ID (P, Q, or X number) plus a reference number \t|\n",
    "| label \t| line number: obverse/reverse, column number, line number (e.g. o ii 16')                                                          \t|\n",
    "| lemma      \t| Sumerian words in lemmatized form (e.g. lugal[king]N); for unlemmatized words the raw transliteration is taken                                                                                  \t|\n",
    "| base      \t| Sumerian words in original spelling, but without morphological prefixes or suffixes   |\n",
    "| extent | (for missing data): how many lines or columns (restricted vocabulary) are missing|\n",
    "| scope | (for missing data): what is missing - line, column, face, or surface (restricted vocabulary) |\n",
    "\n",
    "There are various types of missing data, represented in different ways. A word that is present, but not lemmatized is represented in its transliterated form, followed by [NA]NA (that is: Guideword and part of speech (POS) are both NA). Words that are partly or entirely illegible on the original document are by definition unlemmatized and are handled the same way.\n",
    "\n",
    "Lines or multiple lines that are missing are indicated in the fields `extent` and `scope`. `Extent` gives the number of missing lines (or missing columns, etc). The restricted vocabulary includes numbers and the words 'n' (unknown), 'beginning', and 'rest'. `Scope` indicates the scope of the missing text: line, column, obverse, etc.\n",
    "\n",
    "| type         | how represented                     |\n",
    "|-----------\t|------------------------------------------------------------------------------------------------------------------------------------------------------\t|\n",
    "| words with unknown lemmatization| siki-siki[NA]NA |\n",
    "| illegible words | x[NA]NA |\n",
    "| known number of missing lines \t|extent: '5' scope: 'line' |\n",
    "| unknown number of missing lines\t|extent: 'n' scope: 'line |\n",
    "| two missing columns  | extent: '2' scope: 'column'|\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in and structuring the data\n",
    "\n",
    "Choose a file (from the `csv` files in `data/raw`) and create a Dataframe in Pandas. Note that the field `extent` has to be read as string (not float); this is dealt with with the argument `dtype={'extent': object}`. See [stackoverflow](http://stackoverflow.com/questions/13293810/import-pandas-dataframe-column-as-string-not-int)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: ob_lists_wood.csv\n"
     ]
    }
   ],
   "source": [
    "filename = input('Filename: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = '../data/raw/' + filename\n",
    "df = pd.read_csv(file, dtype={'extent': object}).drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## id_text and line\n",
    "The variable `id_line` contains the text ID plus a reference. The reference may be to a column, a line, or a set of broken lines. This reference is turned into an integer and put in the variable `line`. The variables `id_text` and `line` are then used to sort the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['line'] = [int(re.sub('.+\\.', '', line)) for line in df['id_line']] #create a line number for sorting\n",
    "df = df.sort_values(['id_text', 'line']).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `skip` variable\n",
    "\n",
    "The variable `skip` is used to compute the distance between two lines in the data set. If a line has data (in `label`, `lemma`, and `base`) `skip` = 0. If, however, the original text has 5 missing lines, there will be a separate row, where `skip` is 5. If there is a gap in the text of unknown length, `skip` will be NaN.\n",
    "\n",
    "The `skip` variable works as follows (simplified data representation):\n",
    "\n",
    "|`label` | `lemma` | `skip` | `line`\n",
    "|--------|----------|--------|------|\n",
    "| o ii 4 | gigir[chariot]N    | 0| 43 |\n",
    "| o ii 5 | sahargi[dustguard]N gigir[chariot]N | 0| 44 |\n",
    "| NaN     | NAN     | 5 | 45 |\n",
    "| o ii 11 | margida[wagon]N | 0| 46 |\n",
    "\n",
    "\n",
    "The distance between the `margida[wagon]N` line and the `gigir[chariot]N` line is 7 (`line`₂ - `line`₁ + `skip`₁:₂ -1).\n",
    "\n",
    "The variable `skip` is computed from the [ORACC](http://oracc.org) variables `extent` and `scope`, which are part of the so-called \\$-line conventions. These conventions are explained in more detail [here](http://oracc.org/doc/help/editinginatf/primer/structuretutorial). A 'strict' \\$-line uses a limited vocabulary to describe the preservation or state of the object on which the text is written. Examples of strict \\$-lines are:,\n",
    "* \\$ beginning of column missing\n",
    "* \\$ 7 lines traces\n",
    "\n",
    "In these examples '7' and 'beginning' are the `extent`; 'column' and 'line' are `scope` ('missing' and 'traces' are `state`. The variable 'state' is ignored here - treating 'missing', 'broken', 'traces', etc. all as absence of data).\n",
    "\n",
    "If lines are missing the `extent` variable will indicate the number of missing lines or columns. A line with data has `extent` NaN.\n",
    "\n",
    "The variable `skip` is computed as follows:\n",
    "\n",
    "* if the line has data (in `label`, `lemma`, and `base`) `skip` = 0\n",
    "* if `scope` == 'column', or anything other than 'line', `skip` = NaN\n",
    "* if `extent` is a digit, `skip` is the integer version of that digit\n",
    "* if `extent` is 'n' or 'beginning' (or anything other than a digit), `skip` = NaN\n",
    "\n",
    "Because NaN cannot be used in a column of integers, `skip` will become a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.extent = df.extent.fillna('0')\n",
    "df['skip'] = [int(n) if n.isdigit() else np.NaN for n in df.extent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `scope` may include 'line', 'column', 'obverse', etc. Only if scope is `line` the variable 'extent' is meaningful (if, say, 2 columns are missing, 'extent' is '2' but should be NaN because we do not know how many lines those 2 columns represent). If `scope` is NaN `extent` is '0' and should remain so. After this operation the column 'scope' can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.skip = [df.skip[i] if df.scope[i] in ['line', np.NaN] else np.NaN for i in range(len(df))] # line to be activated when introducing `skip`\n",
    "df = df.drop(['scope', 'extent'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Expressions\n",
    "A line in a lexical text may contain more than one word. Usually a list is divided into sections by keyword, for instance:\n",
    "\n",
    "| text                \t| translation                      \t|\n",
    "|---------------------\t|----------------------------------\t|\n",
    "| {ŋeš}gigir          \t| chariot                          \t|\n",
    "| {ŋeš}e₂ gigir       \t| chariot cabin                    \t|\n",
    "| {ŋeš}e₂ usan₃ gigir \t| storage box for the chariot whip \t|\n",
    "| {ŋeš}gaba gigir     \t| breastwork of a chariot          \t|\n",
    "\n",
    "In the comparison between different versions of the list the individual words are less interesting than the *entries*, that is: the sequence of words in a single line. In order to look at entries (rather than words), words in an entry are connected by underscores (_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['entry'] = df['lemma']\n",
    "df['entry'] = df['entry'].str.replace(' ', '_')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing data to R\n",
    "\n",
    "The first step in passing our data from Python to R is converting the Python null value (`NaN`) to the R null value (`NA`). We do this by converting all `NaN` values in character/string columns to the intermediate value `unknown` and all `NaN` values in our numeric `skip` column to the intermediate value `999`.\n",
    "\n",
    "We will then convert both of these values to `NA` after passing the data to R.\n",
    "\n",
    "The directory `data/pass` is used for passing data from Python to R and vv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.skip = df.skip.fillna(999)\n",
    "df = df.fillna('unknown')\n",
    "df.to_csv(\"../data/pass/\" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
