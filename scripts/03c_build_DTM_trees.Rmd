---
title: "03c_build_DTM_trees.Rmd"
author: "Erin Becker"
date: "April 18, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_knit$set(root.dir = "~/Box Sync/digital-humanities-phylogenetics/")
```

```{r}
#### Setup
## If you haven't already, install and load the required packages. 

# install.packages(c("ggplot2", "ggdendro", "reshape", "scales", "RColorBrewer", "gtools", "reshape2"))

#source("http://bioconductor.org/biocLite.R")
#biocLite("BiocInstaller")
#biocLite("phangorn")

library(ggplot2)
library(ggdendro)
library(reshape)
library(reshape2)
library(scales)
library(RColorBrewer)
library(gtools)
library(phangorn)
library(phytools)
```


### Read in the cleaned DTM

In the previous script, we cleaned the DTM. Read in the cleaned DTM and preview. 

```{r}
dtm_df = read.csv("data/dtm_data/dtm_df_clean.csv", stringsAsFactors = FALSE)
head(dtm_df)
```

### Move document names to row names
We need to ensure that we do not include the document names in the columns that are
being bootstrapped. It is unlikely that with so many columns (over 2400) that the
document name column will be sampled many times, but if it is, it could potentially
affect the results (as the document names do not contribute to the distance matrix).

We can solve this problem by moving the document names column to row names of the data frame. 

```{r}
row.names(dtm_df) = dtm_df$document
dtm_df$document = NULL
dtm_df$X = NULL
head(dtm_df)
```

### Create bootstrapped DTMs

We now create many different versions of the DTM by resampling from within the DTM. This will enable us to evaluate the quality of each branching point on our dendrograms
based on the fraction of dendrograms generated from these bootstrapped DTMs that 
replicate that branching point. Note that eval is set to FALSE so that these don't regenerate every time the file is knit.

```{r echo = TRUE, eval = FALSE}

dir.create("data/dtm_data/bootstrap/dtms", recursive = TRUE)

# save row names as a vector so we can add those back in as a document column for
# when we melt the data frame later
row_names = row.names(dtm_df)

for(i in 1:1000) {
  filename = paste0("data/dtm_data/bootstrap/dtms/dtm_df_", i, ".csv")
  dtm_bootstrap = sample(dtm_df, replace = TRUE)
  dtm_bootstrap$document = row_names
  write.csv(dtm_bootstrap, filename, row.names = FALSE, quote = FALSE)
}
```

### Build distance matrix based on entry overlap across document pairs

To help understand the scale and pattern of shared entries across the corpus, we want to build a dataframe to store the absolute number of (unique) shared entries between each document pair. This will not count duplicated entries within one of the two documents. We will then use that dataframe to calculate a distance matrix using the formula: 

similarity = (number of shared entries / length of shortest text in pair)

distance = 1 - similarity

First, we define some functions:

```{r echo = TRUE}
# function to extract entries of any specified document
get_entries = function(df, doc) {
  as.character(df[which(df$document == doc &
                          df$value == 1),]$variable)
}

# function to get number of shared entries for any specified document pair
get_shared_entries = function(df, doc1, doc2) {
  doc1_entries = get_entries(df, doc1)
  doc2_entries = get_entries(df, doc2)
  length(which(doc1_entries %in% doc2_entries))
}
```

We next convert the dtm dataframe from wide format to long format. In wide format, each document is a row and each entry is a column. There are as many observed values per row as their are entries. In long format, each document/entry combination is a row and there is only one observed value per row.

We then initialize an empty dataframe (all_pairs), compute the number of shared entries for each
document pair and add that to our dataframe (all_pairs), reorder the new dataframe
so that document pairs with the most shared entries are on top, add back in our `num_entries` column (we had to remove it to build the long format dataframe) and calulate our similarity metric for each document pair. We do this in a single code chunk so that we can make it a function and run it with a for loop for each of our bootstrapped dtms.

```{r}

calculate_distance = function(dtm) {
  
  # add document rownames
  row.names(dtm) = row_names
  
  melted_dtm = melt(dtm)

  # initialize an empty dataframe
  docs = as.character(unique(melted_dtm$document))

  all_pairs = permutations(length(docs), 2, docs, repeats.allowed = T)
  all_pairs = as.data.frame(all_pairs, stringsAsFactors = FALSE)
  all_pairs$doc1 = all_pairs$V1
  all_pairs$doc2 = all_pairs$V2
  all_pairs$V1 = NULL
  all_pairs$V2 = NULL
  all_pairs$num_shared_entries = NA

  # compute the number of shared entries for each pair and add to dataframe
  for(i in 1:nrow(all_pairs)) {
    all_pairs[i,]$num_shared_entries = 
      get_shared_entries(melted_dtm, all_pairs[i,]$doc1, all_pairs[i,]$doc2)
  }

  # reorder the new data frame so that the document pairs with 
  # the most shared entries are on top
  all_pairs = all_pairs[order(-all_pairs$num_shared_entries),] 

  # add back in num_entries column but exclude document column from sums
  dtm$num_entries = rowSums(dtm[-which(names(dtm) == "document")])

  # calculate similarity metric and add to dataframe
  all_pairs$similarity = NA
  all_pairs$distance = NA

  get_num_entries = function(df, doc) {
    df[which(df$document == doc),]$num_entries 
  }

  for(i in 1:nrow(all_pairs)) {
    doc1_entries = get_num_entries(dtm, all_pairs[i,]$doc1)
    doc2_entries = get_num_entries(dtm, all_pairs[i,]$doc2)
  
    all_pairs[i,]$similarity = 
      all_pairs[i,]$num_shared_entries / min(doc1_entries, doc2_entries)
  
    all_pairs[i,]$distance = 1 - all_pairs[i,]$similarity
  }

  return(all_pairs)
  
}
```

Run the function defined above for all of the bootstrapped DTMs and write out a new set of output files containing the calculated distance matrix for each DTM.

```{r echo = TRUE, eval = FALSE}

dir.create("data/dtm_data/bootstrap/distance_matrices", recursive = TRUE)

for(i in 1:1000) {
  filename_out = paste0("data/dtm_data/bootstrap/distance_matrices/all_pairs_", i, ".csv")
  filename_in = paste0("data/dtm_data/bootstrap/dtms/dtm_df_", i, ".csv")
  # read in one of the bootstrapped dtms
  dtm = read.csv(filename_in, stringsAsFactors = FALSE)
  all_pairs = calculate_distance(dtm)
  write.csv(all_pairs, filename_out)
}
```

Compute a neighbor joining tree for each of the calculated distance matrices and write out the tree file.

```{r echo = TRUE, eval = FALSE}
# Build neighbor joining tree for each distance matrix
for(i in 21:1000) {
  filename_in = paste0("data/dtm_data/bootstrap/distance_matrices/all_pairs_", i, ".csv")
  filename_out = paste0("data/dtm_data/bootstrap/trees/dtm_nj_", i, ".tre")
  all_pairs = read.csv(filename_in, row.names = 1)
  all_pairs_matrix = data.matrix(acast(all_pairs, doc1 ~ doc2, 
                                       value.var='distance', fun.aggregate = sum, margins = FALSE))
  treeNJ  <- NJ(dist(all_pairs_matrix))
  write.tree(treeNJ, filename_out)
}
```

Compute a consensus tree for all generated tree files.

```{r echo = TRUE, eval = FALSE}

trees = list()

for(i in 1:1000) {
  filename_in = paste0("data/dtm_data/bootstrap/trees/dtm_nj_", i, ".tre")
  name = paste0("tree_", i)
  tree = read.tree(filename_in)
  trees[[name]] <- tree
  }

consensus_50 = consensus(trees, p = 0.5)
write.tree(consensus_50, "data/dtm_data/bootstrap/consensus_trees/consensus_50.tre")

consensus_75 = consensus(trees, p = 0.75)
write.tree(consensus_75, "data/dtm_data/bootstrap/consensus_trees/consensus_75.tre")
```

```{r}

class(trees) = "multiPhylo"

#avg_tree = averageTree(trees)
#write.tree(avg_tree, "data/dtm_data/bootstrap/consensus_trees/avg_tree.tre")
mcc_tree = maxCladeCred(trees)
write.tree(mcc_tree, "data/dtm_data/bootstrap/consensus_trees/mcc_tree.tre")

#pdf("data/dtm_data/bootstrap/consensus_trees/mcc_tree.pdf")
#ggtree(mcc_tree) + geom_tiplab()
#dev.off()

#pdf("data/dtm_data/bootstrap/consensus_trees/avg_tree.pdf")
#ggtree(avg_tree) + geom_tiplab()
#dev.off()
```