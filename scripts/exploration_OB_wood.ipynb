{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Babylonian Lists of Trees and Wooden Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to research questions and analysis plan\n",
    "\n",
    "We are interested in understanding relationships among extant versions of lexical texts. Patterns in similarity of these texts may provide important information about text provenance and/or routes of influence from one geographical area onto another. \n",
    "\n",
    "We are also interested in understanding the patterns by which lexical texts evolved and changed. \n",
    "\n",
    "In comparing versions of a lexical text we may think of four types of features: \n",
    "\n",
    "1) presence or absence of entries  \n",
    "2) order of entries within a section  \n",
    "3) order of sections in a document  \n",
    "4) spelling of words  \n",
    "\n",
    "The following sections will explore these four features independently and in combination to uncover patterns of similarity among documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to dataset and data structure\n",
    "\n",
    "This notebook uses data from the Digital Corpus of Cuneiform Lexical Texts ([DCCLT](http://oracc.org/dcclt)) derived from parsed JSON files. For the JSON output from the Open Richly Annotated Cuneiform Corpus ([ORACC](http://oracc.org)) see the [ORACC Open Data documentation](http://oracc.museum.upenn.edu/doc/opendata/index.html).  \n",
    "\n",
    "The JSON files are parsed with the notebook `json_corpus.iynb` in https://github.com/niekveldhuis/JSON-test. This notebook takes an input file, identifying the text IDs of the documents to be parsed. The input file is [ob_lists_wood.txt](https://github.com/ErinBecker/digital-humanities-phylogenetics/blob/master/data/text_ids/ob_lists_wood.txt). \n",
    "\n",
    "The input file lists all the Text IDs of Old Babylonian lists of trees and wooden objects currently in [DCCLT](http://oracc.org/dcclt), as well as the composite text of the [Nippur version](http://oracc.org/Q000039). Text IDs consist of a P plus a six-digit number (commonly referred to as P-number) that is recognized by [ORACC](http://oracc.org) and by the Cuneiform Digital Library Initiative ([CDLI](http://cdli.ucla.edu)) and that has become the de-facto standard in Assyriology. [CDLI](http://cdli.ucla.edu) provides metadata (provenience, period, publication, museum number, etc) for each text. Composite text IDs consist of a Q plus a six-digit number (for instance Q000039). Texts that have not (yet) been cataloged in [CDLI](http://cdli.ucla.edu) receive a (temporary) six-digit X number.\n",
    "\n",
    "The raw data are placed in the directory [data/raw](https://github.com/ErinBecker/digital-humanities-phylogenetics/tree/master/data/raw). Each document has a separate file named dcclt_P######.txt (or dcclt_Q######.txt). These are comma-separated files with the following fields: \n",
    "\n",
    "| field         | description                     |\n",
    "|-----------\t|------------------------------------------------------------------------------------------------------------------------------------------------------\t|\n",
    "| id_line   \t| consists of a text ID (P, Q, or X numebr) plus a reference number \t|\n",
    "| label \t| line number: obverse/reverse, column number, line number (e.g. o ii 16')                                                          \t|\n",
    "| lemma      \t| Sumerian words in lemmatized form (e.g. lugal[king]N); for unlemmatized words the raw transliteration is taken                                                                                  \t|\n",
    "| base      \t| Sumerian words in original spelling, but without morphological prefixes or suffixes   |\n",
    "| extent | (for missing data): how many lines or columns (restricted vocabulary) are missing|\n",
    "| scope | (for missing data): what is missing - line, column, face, or surface (restricted vocabulary) |\n",
    "\n",
    "There are various types of missing data, represented in different ways. A word that is present, but not lemmatized is represented in its transliterated form, followed by [NA]NA (that is: Guideword and POS are both NA). Words that are partly or entirely illegible on the original document are by definition unlemmatized and are handled the same way.\n",
    "\n",
    "Lines or multiple lines that are missing are indicated in the fields `extent` and `scope`. `Extent` gives the number of missing lines (or missing columns, etc). The restricted vocabulary includes numbers and the words 'n' (unknown), 'beginning', and 'rest'. `Scope` indicates the scope of the missing text: line, column, obverse, etc.\n",
    "\n",
    "| type         | how represented                     |\n",
    "|-----------\t|------------------------------------------------------------------------------------------------------------------------------------------------------\t|\n",
    "| words with unknown lemmatization| siki-siki[NA]NA |\n",
    "| illegible words | x[NA]NA |\n",
    "| known number of missing lines \t|extent: '5' scope: 'line' |\n",
    "| unknown number of missing lines\t|extent: 'n' scope: 'line |\n",
    "| two missing columns  | extent: '2' scope: 'column'|\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import rpy2.ipython\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in and structuring the data\n",
    "Open file `obwood.csv` and create a Dataframe in Pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = '../data/ob_lists_wood.csv'\n",
    "df = pd.read_csv(file).drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## id_text and line\n",
    "The variable `id_line` contains the text ID plus a reference. The reference may be to a column, a line, or a set of broken lines. The text ID is put in the variable `id_text` and the reference is turned into an integer and put in the variable `line`. The variables `id_text` and `line` are then used to sort the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['id_text'] = df['id_line'].str[:7]\n",
    "df['line'] = [int(re.sub('.+\\.', '', line)) for line in df['id_line']] #create a line number for sorting\n",
    "df = df.sort_values(['id_text', 'line']).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `skip` variable\n",
    "\n",
    "The variable `skip` is used to compute the distance between two lines in the data set. If a line has data (in `label`, `lemma`, and `base`) `skip` = 0. If, however, the original text has 5 missing lines, there will be a separate row, where `skip` is 5. If there is a gap in the text of unknown length, `skip` will be NaN.\n",
    "\n",
    "The `skip` variable works as follows (simplified data representation):\n",
    "\n",
    "|`label` | `lemma` | `skip` | `line`\n",
    "|--------|----------|--------|------|\n",
    "| o ii 4 | gigir[chariot]N    | 0| 43 |\n",
    "| o ii 5 | sahargi[dustguard]N gigir[chariot]N | 0| 44 |\n",
    "| NaN     | NAN     | 5 | 45 |\n",
    "| o ii 11 | margida[wagon]N | 0| 46 |\n",
    "\n",
    "\n",
    "The distance between the `margida[wagon]N` line and the `gigir[chariot]N` line is 7 (`line`₂ - `line`₁ + `skip`₁:₂ -1).\n",
    "\n",
    "The variable `skip` is computed from the [ORACC](http://oracc.org) variables `extent` and `scope`, which are part of the so-called \\$-line conventions. These conventions are explained in more detail [here](http://oracc.org/doc/help/editinginatf/primer/structuretutorial). A 'strict' \\$-line uses a limited vocabulary to describe the preservation or state of the object on which the text is written. Examples of strict \\$-lines are:,\n",
    "* \\$ beginning of column missing\n",
    "* \\$ 7 lines traces\n",
    "\n",
    "In these examples '7' and 'beginning' are the `extent`; 'column' and 'line' are `scope` ('missing' and 'traces' are `state`. The variable 'state' is ignored here - treating 'missing', 'broken', 'traces', etc. all as absence of data).\n",
    "\n",
    "If lines are missing the `extent` variable will indicate the number of missing lines or columns. A line with data has `extent` NaN.\n",
    "\n",
    "The variable `skip` is computed as follows:\n",
    "\n",
    "* if the line has data (in `label`, `lemma`, and `base`) `skip` = 0\n",
    "* if `scope` == 'column', or anything other than 'line', `skip` = NaN\n",
    "* if `extent` is a digit, `skip` is the integer version of that digit\n",
    "* if `extent` is 'n' or 'beginning' (or anything other than a digit), `skip` = NaN\n",
    "\n",
    "Because NaN cannot be used in a column of integers, `skip` will become a float.\n",
    "\n",
    "## Note to Erin\n",
    "\n",
    "I am proposing here to introduce a new variable `skip` (rather than adjust the variable `extent`). I think that is clearer to outsiders (and to our future selves). I hope the explanation of how it is done is also clearer. I haven't actually done what I describe here - because it would mess with your datastructure. Essentially, everything remains exactly the same, but what was called `extent` will now be called `skip`. Note that the formula for line distance needs more thought. The number of rows that have some `skip` value needs to be subtracted - there should be some smart way of doing that.\n",
    "\n",
    "We could, of course, use `999` instead of `NaN` for `skip` - but I prefer to use `NaN` here and to present the change to `999` as a feature of handing the data off to R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.extent = df.extent.fillna('0')\n",
    "df['skip'] = [int(n) if n.isdigit() else np.NaN for n in df.extent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `scope` may include 'line', 'column', 'obverse', etc. Only if scope is `line` the variable 'extent' is meaningful (if, say, 2 columns are missing, 'extent' is '2' but should be NaN because we do not know how many lines those 2 columns represent). If `scope` is NaN `extent` is '0' and should remain so. After this operation the column 'scope' can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.skip = [df.skip[i] if df.scope[i] in ['line', np.NaN] else np.NaN for i in range(len(df))] # line to be activated when introducing `skip`\n",
    "df = df.drop(['scope', 'extent'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df.loc[df.skip > 0].head()\n",
    "#df.iloc[80:90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Expressions\n",
    "A line in a lexical text may contain more than one word. Usually a list is divided into sections by keyword, for instance:\n",
    "\n",
    "| text                \t| translation                      \t|\n",
    "|---------------------\t|----------------------------------\t|\n",
    "| {ŋeš}gigir          \t| chariot                          \t|\n",
    "| {ŋeš}e₂ gigir       \t| chariot cabin                    \t|\n",
    "| {ŋeš}e₂ usan₃ gigir \t| storage box for the chariot whip \t|\n",
    "| {ŋeš}gaba gigir     \t| breastwork of a chariot          \t|\n",
    "\n",
    "In the comparison between different versions of the list the individual words are less interesting than the *entries*, that is: the sequence of words in a single line. In order to look at entries (rather than words), words in an entry are connected by underscores (_). Since in this case all words are in Sumerian, the language designation (sux:) is removed from the field `entry`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['entry'] = df['lemma']\n",
    "df['entry'] = df['entry'].str.replace(' ', '_')\n",
    "df.head()\n",
    "#df.to_csv(\"../data/ob_lists_wood_w_id_text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by Document\n",
    "The `groupby()` function is used to group the data by document. The function `apply(' '.join)` concatenates the text in the `entries` column, separating them with a white space. The Pandas `groupby()` function results in a series, which is then tranformed into a new Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['entry'] = df['entry'].fillna('')\n",
    "entries_df = df[['id_text', 'line', 'entry']]\n",
    "#entries_df = entries_df.dropna()\n",
    "grouped = entries_df['entry'].groupby(entries_df['id_text']).apply(' '.join).reset_index()\n",
    "by_text_df = pd.DataFrame(grouped)\n",
    "by_text_df = by_text_df.set_index('id_text')\n",
    "by_text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for Niek\n",
    "1) What does the symbol \"~\" mean in the lemmatization? For example in row number one of the df above \"~tree\". \n",
    "\n",
    "NV: This means: \"pertains to\" and is used for words of vague or unclear semantics.\n",
    "\n",
    "2) Can you explain what's going on with the \"line\" column? It appears to start at an arbitrary number for each document.  \n",
    "\n",
    "NV: The field `line` is derived directly from the field `id_line`, minus the `id_text` (P-number) element. `Id_line` is a string but `line` is an integer, used to keep the lines in the right order.  \n",
    "\n",
    "3) Why do the first several entries in the DTM start with a number? It looks like these are all words that are unlematized. What do the numbers refer to? Is \"10[na]_na\" different from \"11[na]_na\"?\n",
    "\n",
    "NV: Some of these entries come from P251686 - and they indicate a problem we hadn't seen before. This is a tablet that combines a list of wooden objects with a metrological table. The metrological table shouldn't be here - there are, I believe, a few other such instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Analysis based on order of entries within a section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining sections\n",
    "\n",
    "The first step in analyzing order of entries within a section must be defining what constitutes a section. We've discussed three methods for defining sections. \n",
    "\n",
    "**1) Expert definition.**  \n",
    "**2) Automated based on composite text (eg. Q000039).**  \n",
    "**3) Automated based on entry proximity.** \n",
    "\n",
    "These methods are described more below:\n",
    "\n",
    "\n",
    "**1) Expert definition.**  \n",
    "Expert manually determines section boundaries based on knowledge of text type. This approach is the most sound, but does not scale. If we relied on this method, we would not be able to use our workflow with other collections of lexical lists without a time-intensive manual step. This method will be set aside for now. We may, however, want to leave users (assuming there ever are any) the option to read in and use their own section definitions for downstream analyses.  \n",
    "\n",
    "**2) Automated based on composite text.**  \n",
    "A composite text is read in and breakpoints in the text are determined based on fuzzy matching of similar words (either in base or lemma). Entries between those breakpoints are assumed to belong to the same section. This method would need to be tested and perhaps supervised to ensure that nonsense sections (a collection of words that don't really belong to any section) aren't grouped together. It may also miss sections that are based not on similarity of words (e.g. \"palm\") but similarity of object type or use (e.g. \"bowl\", \"spoon\", \"cup\"), unless that section is between two sections that are picked up by this method.  \n",
    "\n",
    "Based on the following (artificial) Q text, this method should lead to three sections. Lines 1-4 (related to palm), lines 5-8 (related to polar), and lines 9-14 (related to tree). \n",
    "\n",
    "*1) ŋešnimbar[palm]N*  \n",
    "*2) ŋešnimbar[palm]N sux:suhuš[offshoot]N*  \n",
    "*3) deg[collect]V/t sux:ŋešnimbar[palm]N*  \n",
    "*4) niŋkiluh[broom]N sux:ŋešnimbar[palm]N*  \n",
    "*5) asal[poplar]N*  \n",
    "*6) asal[poplar]N sux:kur[mountain]N*  \n",
    "*7) asal[poplar]N sux:dug[good]V/i*  \n",
    "*8) numun[seed]N sux:asal[poplar]N*  \n",
    "*9) ilur[tree]N*  \n",
    "*10) ad[bush]N*  \n",
    "*11) kišig[acacia]N*  \n",
    "*12) kišighar[tree]N*  \n",
    "*13) samazum[tree]N*  \n",
    "*14) peškal[tree]N*  \n",
    "\n",
    "**3) Automated based on entry proximity**  \n",
    "Lines that always apear within a certain (small) distance from each other could be considered to be part of the same section. Sections are defined based on entire corpus, not just a composite text. (eg. moving window, ~6-8 lines, middle in target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2) Automated based on composite text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our strategy for implementing this method is the following:  \n",
    "1) First, calculate the degree of similarity between any two adjacent entries in the source/composite document. This similarity can take two independent forms:\n",
    "- shared character strings in the Sumerian (citation form) or the English (guideword)\n",
    "- shared entire words (citation forms and/or guidewords)  \n",
    "\n",
    "2) Next, determine appropriate cutoff values for similarity that distinguish between sections.    \n",
    "3) Next, automatically assign a name to each section for ease of reference.  \n",
    "4) Finally, output a file containing the entries present in each section. \n",
    "\n",
    "We can then use these section definitions to analyze similarities and differences between texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in passing our data from Python to R is converting the Python null value (`NaN`) to the R null value (`NA`). We do this by converting all `NaN` values in character/string columns to the intermediate value `unknown` and all `NaN` values in our numeric `skip` column to the intermediate value `999`.\n",
    "\n",
    "We will then convert both of these values to `NA` after passing the data to R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.skip = df.skip.fillna(999)\n",
    "df = df.fillna('unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then bring the data into R, convert the `lemma`, `base` and `entry` columns into character strings and check that all columns are represented correctly in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R -i df\n",
    "\n",
    "df$lemma = as.character(df$lemma)\n",
    "df$base = as.character(df$base)\n",
    "df$entry = as.character(df$entry)\n",
    "str(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%R\n",
    "#source(\"http://bioconductor.org/biocLite.R\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:186: RRuntimeWarning: Bioconductor version 3.4 (BiocInstaller 1.24.0), ?biocLite for help\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "//anaconda/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:186: RRuntimeWarning: A new version of Bioconductor is available after installing the most recent\n",
      "  version of R; see http://bioconductor.org/install\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "//anaconda/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:186: RRuntimeWarning: BioC_mirror: https://bioconductor.org\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "//anaconda/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:186: RRuntimeWarning: Using Bioconductor 3.4 (BiocInstaller 1.24.0), R 3.3.2 (2016-10-31).\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "//anaconda/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:186: RRuntimeWarning: Installing package(s) ‘Rlibstree’\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "//anaconda/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:186: RRuntimeWarning: trying URL 'https://bioconductor.org/packages/3.4/extra/src/contrib/Rlibstree_0.3-2.tar.gz'\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "//anaconda/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:186: RRuntimeWarning: Content type 'application/x-gzip'\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "//anaconda/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:186: RRuntimeWarning:  length 912713 bytes (891 KB)\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "//anaconda/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:186: RRuntimeWarning: =\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "//anaconda/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:186: RRuntimeWarning: \n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "//anaconda/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:186: RRuntimeWarning: downloaded 891 KB\n",
      "\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "//anaconda/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:186: RRuntimeWarning: \n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "//anaconda/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:186: RRuntimeWarning: The downloaded source packages are in\n",
      "\t‘/private/var/folders/7g/r8_n81y534z0vy5hxc6dx1t00000gn/T/RtmpA5WJDp/downloaded_packages’\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "//anaconda/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:186: RRuntimeWarning: Old packages: 'broom', 'caret', 'cluster', 'glmnet', 'lme4', 'mgcv', 'nlme',\n",
      "  'pbdZMQ', 'pbkrtest', 'psych', 'quantreg', 'rmarkdown', 'selectr', 'SparseM',\n",
      "  'survival', 'tidyr', 'tidyverse', 'viridis'\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Update all/some/none? [a/s/n]: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%R\n",
    "#library(BiocInstaller)\n",
    "#biocLite(\"Rlibstree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(\"Rlibstree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Analysis based on presence/absence of entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Term Matrix\n",
    "Transform the DataFrame into a Document Term Matrix (DTM) by using CountVectorizer. This function uses a Regular Expression (token_pattern) to indicate how to find the beginning and end of token. In the current Dataframe entries are separated from each other by a white space. The expression `r.[^ ]+` means: any combination of characters, except the space.\n",
    "\n",
    "The output of the CountVectorizer (`dtm`) is not in a human-readable format. It is transformed into another DataFrame, with `id_text` as index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(analyzer='word', token_pattern=r'[^ ]+')\n",
    "dtm = cv.fit_transform(by_text_df['entry'])\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), columns = cv.get_feature_names(), index = by_text_df.index.values)\n",
    "dtm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the DTM\n",
    "Each document in the DTM may be understood as a vector, which allows for various kinds of computations, such as distance or cosine-similarity. \n",
    "\n",
    "It is important to recall that the DTM does not preserve information about the order of entries.\n",
    "\n",
    "It is also important to realize that the documents in this analysis of are of very different length (from 1 to 750 entries), with more than half of the documents 3 lines or less. The composite text from Nippur is by far the longest document and will dominate any comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_length = dtm_df.sum(axis=1)\n",
    "df_length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I'll be doing some analysis in R, whereas Niek will be doing some in Python. We can use both languages in different cells of the same notebook and even pass variables between languages. See tip #21 [here](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/).\n",
    "\n",
    "FYI - if you have difficulty running R cells in a Python notebook using rpy2, try installing\n",
    "rpy2 through conda instead of through pip.  \n",
    "\n",
    "`conda install -c r rpy2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure the R correctly parsed the DTM  \n",
    "- It looks like R doesn't allow variable names to start with a number, thus all entries starting with a number had \"X\" added to the beggining of the entry name.  \n",
    "- R doesn't allow parentheses in variable names, so entries like \"1(ban₂)[na]na\" parsed as \"X1.ban...na.na\".   \n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R -i dtm_df\n",
    "#Import dtm_df from Python\n",
    "\n",
    "# Set cols and rows to sum to not include summary column and row added later\n",
    "# This needs to be in separate cell from addition of summary col and row!\n",
    "cols_to_sum = ncol(dtm_df)\n",
    "rows_to_sum = nrow(dtm_df)\n",
    "\n",
    "#head(dtm_df[,1:10])\n",
    "#str(dtm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check density of DTM\n",
    "\n",
    "Look at distribution of document lengths (number of entries per document).  \n",
    "Look at distribution of entry freqency (number of documents each entry appears in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "dtm_df$num_entries = rowSums(dtm_df[1:cols_to_sum])\n",
    "dtm_df[\"num_occurances\",] = colSums(dtm_df[1:rows_to_sum,])\n",
    "dtm_df[\"num_occurances\",\"num_entries\"] = NA \n",
    "\n",
    "plot(density(dtm_df$num_entries, na.rm = TRUE))\n",
    "table(dtm_df$num_entries, useNA = \"ifany\") #number of documents with each number of entries\n",
    "\n",
    "print(paste(\"There are\", length(which(dtm_df$num_entries >= 10)), \"documents with 10 or more entries.\"))\n",
    "print(paste(\"There are\", length(which(dtm_df$num_entries >= 100)), \"documents with 100 or more entries.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "\n",
    "num_occurances = unlist(dtm_df[\"num_occurances\",])\n",
    "plot(density(num_occurances, na.rm = TRUE))\n",
    "table(num_occurances, useNA = \"ifany\")\n",
    "\n",
    "rare = round(length(which(dtm_df[\"num_occurances\",] <=2))/cols_to_sum*100,2)\n",
    "common = length(which(dtm_df[\"num_occurances\",] >=10))\n",
    "most_common = max(dtm_df[\"num_occurances\",], na.rm = TRUE)\n",
    "most_common_entry = colnames(dtm_df[which(dtm_df[\"num_occurances\",] == most_common)])\n",
    "\n",
    "print(paste0(rare, \"% of entries appear only once or twice across the corpus\"))\n",
    "print(paste(common, \"entries occur in 10 or more documents\"))\n",
    "print(paste(\"including one that occurs\", most_common, \"times across the\", rows_to_sum, \"documents\"))\n",
    "print(paste(\"The most common entry is\", most_common_entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "# Look at some of the most common entries\n",
    "colnames(dtm_df)[which(dtm_df[\"num_occurances\",] >= 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "\n",
    "# currently \"variables\" (entries) are sorted alphabetically, would like sorted by frequency\n",
    "dtm_df = as.matrix((dtm_df > 0) + 0) # Converts to binary presence/absence information\n",
    "dtm_df = dtm_df[,order(colSums(dtm_df), decreasing = TRUE)]\n",
    "dtm_df = as.data.frame(dtm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "\n",
    "# Need to recaculate number of occurances, as was converted to binary. \n",
    "dtm_df$num_entries = rowSums(dtm_df[1:cols_to_sum])\n",
    "dtm_df[\"num_occurances\",] = colSums(dtm_df[1:rows_to_sum,])\n",
    "dtm_df[\"num_occurances\",\"num_entries\"] = NA \n",
    "\n",
    "num_occurances = unlist(dtm_df[\"num_occurances\",])\n",
    "most_frequent = max(num_occurances, na.rm = TRUE)\n",
    "most_frequent_entry = colnames(dtm_df[which(dtm_df[\"num_occurances\",] == most_frequent)])\n",
    "\n",
    "print(paste(table(num_occurances)[1], \"entries appear in only one document\"))\n",
    "print(paste(\"The entry that appears in the most documents is\", most_frequent_entry))\n",
    "\n",
    "table(num_occurances, useNA = \"ifany\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "# These entries appear in at least 10 different documents.\n",
    "colnames(dtm_df)[which(dtm_df[\"num_occurances\",] >=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "dtm_df$document = row.names(dtm_df) #add document names as row names\n",
    "dtm_df = dtm_df[-which(row.names(dtm_df) == \"num_occurances\"),] # remove num_occurances row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#install.packages(\"reshape\")\n",
    "library(reshape)\n",
    "\n",
    "melted_dtm_df = melt(dtm_df)\n",
    "head(melted_dtm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "# http://stackoverflow.com/questions/10397183/heat-map-of-binary-data-using-r-or-python\n",
    "install.packages(\"ggplot2\", lib='/Library/Frameworks/R.framework/Versions/3.2/Resources/library')\n",
    "library(ggplot2)\n",
    "# ggplot(data = melted_dtm_df[150000:160474,], aes(y=document, x=variable, fill=value)) + \n",
    "#   geom_tile() +\n",
    "#   theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 5))\n",
    "\n",
    "#qplot(data=melted_dtm_df, x=variable,y=document, fill=factor(value),\n",
    "#    geom=\"tile\")+scale_fill_manual(values=c(\"0\"=\"lightblue\", \"1\"=\"red\")) +\n",
    "#  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 1), axis.text.y = element_text(size = 3))\n",
    "\n",
    "# Look at a subset\n",
    "# qplot(data = melted_dtm_df[1:10000,], x=variable, y=document, fill=factor(value),\n",
    "#     geom=\"tile\")+scale_fill_manual(values=c(\"0\"=\"lightblue\", \"1\"=\"red\")) +\n",
    "# theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8), axis.text.y = element_text(size = 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Grouping Documents by Entry Similarity  \n",
    "We can use hierarchical clustering with our presence/absence matrix to uncover groups of similar documents. Ideally, we can benchmark these clusters' accuracy in uncovering geographically or chronologically related documents by looking at metadata, but for this collection the metadata may be too sparse to do that benchmarking.  \n",
    "\n",
    "In either case, we can establish a workflow for doing hierarchical clustering and apply that to other datasets with better provenance information to test for cluster utility.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troubleshooting note:  \n",
    "When running R as a magic within Jupyter notebook, running install.packages() leads to the notebook prompting you for a selection. It turns out that this is due to the fact that anaconda actually installs a second R installation and stores installed packages separately from the users \"main\" R installation.  \n",
    "\n",
    "To avoid this issue, run '.libPaths()' within R in your console to find the path where anaconda stores your packages. You can then download binaries from CRAN and put them in that directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "# See Troubleshooting note above\n",
    "\n",
    "install.packages(\"ggdendro\", lib='/Library/Frameworks/R.framework/Versions/3.2/Resources/library')\n",
    "# install.packages(\"ggdendro\", \"/anaconda/lib/R/library\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    ".libPaths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(ggdendro)\n",
    "clusters <- hclust(dist(dtm_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "ggdendrogram(clusters, rotate = TRUE) + theme(axis.text.y = element_text(size = 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding provenience by using ORACC metadata\n",
    "The file `data/metadata/dcclt-eta.csv` contains some metadata, including document provenience when known. We read in this data and add provenience to our DTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "# Bring in metadata\n",
    "ids = read.csv(\"../data/metadata/dcclt_meta.csv\")\n",
    "ids$document = ids$X\n",
    "ids$X = NULL\n",
    "head(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "# Add provenance information to dtm_df\n",
    "dtm_df = merge(dtm_df, ids, by = \"document\")\n",
    "dtm_df$provenience = droplevels(dtm_df)$provenience\n",
    "table(dtm_df$provenience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Add colors to dendrogram by provenance\n",
    "library(scales)\n",
    "numColors = length(levels(factor(dtm_df$provenience)))\n",
    "numColors\n",
    "myPalette = brewer_pal(palette = \"Paired\")(numColors)\n",
    "names(myPalette) = levels(dtm_df$provenience)\n",
    "print(names(myPalette))\n",
    "show_col(myPalette)\n",
    "ggdendrogram(clusters, rotate = TRUE) + \n",
    "theme(axis.text.y = element_text(size = 8, color = myPalette[dtm_df$provenience]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
