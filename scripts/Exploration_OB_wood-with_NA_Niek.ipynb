{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Babylonian Lists of Trees and Wooden Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to research questions and analysis plan\n",
    "\n",
    "We are interested in understanding relationships among extant versions of lexical texts. Patterns in similarity of these texts may provide important information about text provenence and/or routes of influence from one geographical area onto another. \n",
    "\n",
    "We are also interested in understanding the patterns by which lexical texts evolved and changed. \n",
    "\n",
    "In comparing versions of a lexical text we may think of four types of features: \n",
    "\n",
    "1) presence or absence of entries  \n",
    "2) order of entries within a section  \n",
    "3) order of sections in a document  \n",
    "4) spelling of words  \n",
    "\n",
    "The following sections will explore these four features independently and in combination to uncover patterns of similarity among documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to dataset and data structure\n",
    "\n",
    "This notebook uses data from [DCCLT](http://oracc.org/dcclt) derived from parsed JSON files. For the [ORACC](http://oracc.org) JSON output see the [ORACC Open Data documentation](http://oracc.museum.upenn.edu/doc/opendata/index.html). The JSON files are parsed with the notebook `json_corpus.iynb` in https://github.com/niekveldhuis/JSON-test. This notebook takes an input file, identifying the text IDs of the documents to be parsed. The input file is [ob_lists_wood.txt](https://github.com/ErinBecker/digital-humanities-phylogenetics/blob/master/data/text_ids/ob_lists_wood.txt). \n",
    "\n",
    "The input file lists all the Text IDs of Old Babylonian lists of trees and wooden objects currently in [DCCLT](http://oracc.org/dcclt), as well as the composite text of the Nippur version. Text IDs consist of a P plus a six-digit number (commonly referred to as P-number) that is recognized by [ORACC](http://oracc.org) and by [CDLI](http://cdli.ucla.edu) and that has become the de-facto standard in Assyriology. [CDLI](http://cdli.ucla.edu) provides metadata (provenience, period, publication, museum number, etc) for each text. Composite text IDs consist of a Q plus a six-digit number (for instance Q000039). Texts that have not (yet) been cataloged in [CDLI](http://cdli.ucla.edu) receive a (temporary) six-digit X number.\n",
    "\n",
    "The raw data are placed in the directory [data/raw](https://github.com/ErinBecker/digital-humanities-phylogenetics/tree/master/data/raw). Each document has a separate file named dcclt_P######.txt (or dcclt_Q######.txt). These are comma-separated files with the following fields: \n",
    "\n",
    "| field         | description                     |\n",
    "|-----------\t|------------------------------------------------------------------------------------------------------------------------------------------------------\t|\n",
    "| id_line   \t| consists of a text ID (P, Q, or X numebr) plus a reference number \t|\n",
    "| label \t| line number: obverse/reverse, column number, line number (e.g. o ii 16')                                                          \t|\n",
    "| lemma      \t| Sumerian words in lemmatized form (e.g. lugal[king]N); for unlemmatized words the raw transliteration is taken                                                                                  \t|\n",
    "| base      \t| Sumerian words in original spelling, but without morphological prefixes or suffixes   |\n",
    "| extent | (for missing data): how many lines or columns (restricted vocabulary) are missing|\n",
    "| scope | (for missing data): what is missing - line, column, face, or surface (restricted vocabulary) |\n",
    "\n",
    "There are various types of missing data, represented in different ways. A word that is present, but not lemmatized is represented in its transliterated form, followed by [NA]NA (that is: Guideword and POS are both NA). Words that are partly or entirely illegible on the original document are by definition unlemmatized and are handled the same way.\n",
    "\n",
    "Lines or multiple lines that are missing are indicated in the fields `extent` and `scope`. `Extent` gives the number of missing lines (or missing columns, etc). The restricted vocabulary includes numbers and the words 'n' (unknown), 'beginning', and 'rest'. `Scope` indicates the scope of the missing text: line, column, obverse, etc.\n",
    "\n",
    "| type         | how represented                     |\n",
    "|-----------\t|------------------------------------------------------------------------------------------------------------------------------------------------------\t|\n",
    "| words with unknown translation| siki-siki[NA]NA |\n",
    "| illegible words | x[NA]NA |\n",
    "| known number of missing lines \t|extent: '5' scope: 'line' |\n",
    "| unknown number of missing lines\t|extent: 'n' scope: 'line |\n",
    "| two missing columns  | extent: '2' scope: 'column'|\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import rpy2.ipython\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in and structuring the data\n",
    "Open file `obwood.csv` and create a Dataframe in Pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = '../data/ob_lists_wood.csv'\n",
    "df = pd.read_csv(file).drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_line</th>\n",
       "      <th>label</th>\n",
       "      <th>lemma</th>\n",
       "      <th>base</th>\n",
       "      <th>extent</th>\n",
       "      <th>scope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P117395.2</td>\n",
       "      <td>o 1</td>\n",
       "      <td>ŋešed[key]N</td>\n",
       "      <td>{ŋeš}e₃-a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P117395.3</td>\n",
       "      <td>o 2</td>\n",
       "      <td>pakud[~tree]N</td>\n",
       "      <td>{ŋeš}pa-kud</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P117395.4</td>\n",
       "      <td>o 3</td>\n",
       "      <td>raba[clamp]N</td>\n",
       "      <td>{ŋeš}raba</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P117404.2</td>\n",
       "      <td>o 1</td>\n",
       "      <td>ig[door]N eren[cedar]N</td>\n",
       "      <td>{ŋeš}ig {ŋeš}eren</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P117404.3</td>\n",
       "      <td>o 2</td>\n",
       "      <td>ig[door]N dib[board]N</td>\n",
       "      <td>{ŋeš}ig dib</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_line label                   lemma               base extent scope\n",
       "0  P117395.2   o 1             ŋešed[key]N          {ŋeš}e₃-a    NaN   NaN\n",
       "1  P117395.3   o 2           pakud[~tree]N        {ŋeš}pa-kud    NaN   NaN\n",
       "2  P117395.4   o 3            raba[clamp]N          {ŋeš}raba    NaN   NaN\n",
       "3  P117404.2   o 1  ig[door]N eren[cedar]N  {ŋeš}ig {ŋeš}eren    NaN   NaN\n",
       "4  P117404.3   o 2   ig[door]N dib[board]N        {ŋeš}ig dib    NaN   NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## id_text and line\n",
    "The variable `id_line` contains the text ID plus a reference. The reference may be to a column, a line, or a set of broken lines. The text ID is put in the variable `id_text` and the reference is turned into an integer and put in the variable`line`. The variables `id_text` and `line` are then used to sort the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_line</th>\n",
       "      <th>label</th>\n",
       "      <th>lemma</th>\n",
       "      <th>base</th>\n",
       "      <th>extent</th>\n",
       "      <th>scope</th>\n",
       "      <th>id_text</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P117395.2</td>\n",
       "      <td>o 1</td>\n",
       "      <td>ŋešed[key]N</td>\n",
       "      <td>{ŋeš}e₃-a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P117395</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P117395.3</td>\n",
       "      <td>o 2</td>\n",
       "      <td>pakud[~tree]N</td>\n",
       "      <td>{ŋeš}pa-kud</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P117395</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P117395.4</td>\n",
       "      <td>o 3</td>\n",
       "      <td>raba[clamp]N</td>\n",
       "      <td>{ŋeš}raba</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P117395</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P117404.2</td>\n",
       "      <td>o 1</td>\n",
       "      <td>ig[door]N eren[cedar]N</td>\n",
       "      <td>{ŋeš}ig {ŋeš}eren</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P117404</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P117404.3</td>\n",
       "      <td>o 2</td>\n",
       "      <td>ig[door]N dib[board]N</td>\n",
       "      <td>{ŋeš}ig dib</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P117404</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_line label                   lemma               base extent scope  \\\n",
       "0  P117395.2   o 1             ŋešed[key]N          {ŋeš}e₃-a    NaN   NaN   \n",
       "1  P117395.3   o 2           pakud[~tree]N        {ŋeš}pa-kud    NaN   NaN   \n",
       "2  P117395.4   o 3            raba[clamp]N          {ŋeš}raba    NaN   NaN   \n",
       "3  P117404.2   o 1  ig[door]N eren[cedar]N  {ŋeš}ig {ŋeš}eren    NaN   NaN   \n",
       "4  P117404.3   o 2   ig[door]N dib[board]N        {ŋeš}ig dib    NaN   NaN   \n",
       "\n",
       "   id_text  line  \n",
       "0  P117395     2  \n",
       "1  P117395     3  \n",
       "2  P117395     4  \n",
       "3  P117404     2  \n",
       "4  P117404     3  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['id_text'] = df['id_line'].str[:7]\n",
    "df['line'] = [int(re.sub('.+\\.', '', line)) for line in df['id_line']] #create a line number for sorting\n",
    "df = df.sort_values(['id_text', 'line']).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `extent` variable\n",
    "The variables `extent` and `scope` in ORACC are part of the so-called \\\\$-line conventions, explained in more detail \n",
    "[here](http://oracc.org/doc/help/editinginatf/primer/structuretutorial/). A 'strict' \\\\$-line uses a limited vocabulary to describe the preservation or state of the object on which the text is written. Examples of strict \\\\$-lines are:,\n",
    "* \\\\$ beginning of column missing\n",
    "* \\\\$ 7 lines traces\n",
    "\n",
    "In these examples '7' and 'beginning' are the `extent`; 'column' and 'line' are `scope` ('missing' and 'traces' are `state`. The variable 'state' is ignored here - treating 'missing', 'broken', 'traces', etc. all as absence of data).\n",
    "\n",
    "If lines are missing the `extent` variable will indicate the number of missing lines. A line with data has extent `0`. If the number of missing lines is unknown (as in 'beginning of column missing') `extent` is NaN.\n",
    "\n",
    "First, all lines with data (where `extent` initially is NaN) are set to '0' (as string). Second, all cases where `extent` is a number are converted to integers. If 'extent' is not a number it becomes NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.extent = df.extent.fillna('0')\n",
    "df.extent = [int(n) if n.isdigit() else np.NaN for n in df.extent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `scope` may include 'line', 'column', 'obverse', etc. Only if scope is `line` the variable 'extent' is meaningful (if, say, 2 columns are missing, 'extent' is '2' but should be NaN because we do not know how many lines those 2 columns represent). If `scope` is NaN `extent` is '0' and should remain so. After this operation the column 'scope' can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_line</th>\n",
       "      <th>label</th>\n",
       "      <th>lemma</th>\n",
       "      <th>base</th>\n",
       "      <th>extent</th>\n",
       "      <th>id_text</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P117395.2</td>\n",
       "      <td>o 1</td>\n",
       "      <td>ŋešed[key]N</td>\n",
       "      <td>{ŋeš}e₃-a</td>\n",
       "      <td>0.0</td>\n",
       "      <td>P117395</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P117395.3</td>\n",
       "      <td>o 2</td>\n",
       "      <td>pakud[~tree]N</td>\n",
       "      <td>{ŋeš}pa-kud</td>\n",
       "      <td>0.0</td>\n",
       "      <td>P117395</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P117395.4</td>\n",
       "      <td>o 3</td>\n",
       "      <td>raba[clamp]N</td>\n",
       "      <td>{ŋeš}raba</td>\n",
       "      <td>0.0</td>\n",
       "      <td>P117395</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P117404.2</td>\n",
       "      <td>o 1</td>\n",
       "      <td>ig[door]N eren[cedar]N</td>\n",
       "      <td>{ŋeš}ig {ŋeš}eren</td>\n",
       "      <td>0.0</td>\n",
       "      <td>P117404</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P117404.3</td>\n",
       "      <td>o 2</td>\n",
       "      <td>ig[door]N dib[board]N</td>\n",
       "      <td>{ŋeš}ig dib</td>\n",
       "      <td>0.0</td>\n",
       "      <td>P117404</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_line label                   lemma               base  extent  \\\n",
       "0  P117395.2   o 1             ŋešed[key]N          {ŋeš}e₃-a     0.0   \n",
       "1  P117395.3   o 2           pakud[~tree]N        {ŋeš}pa-kud     0.0   \n",
       "2  P117395.4   o 3            raba[clamp]N          {ŋeš}raba     0.0   \n",
       "3  P117404.2   o 1  ig[door]N eren[cedar]N  {ŋeš}ig {ŋeš}eren     0.0   \n",
       "4  P117404.3   o 2   ig[door]N dib[board]N        {ŋeš}ig dib     0.0   \n",
       "\n",
       "   id_text  line  \n",
       "0  P117395     2  \n",
       "1  P117395     3  \n",
       "2  P117395     4  \n",
       "3  P117404     2  \n",
       "4  P117404     3  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.extent = [df.extent[i] if df.scope[i] in ['line', np.NaN] else np.NaN for i in range(len(df))]\n",
    "df = df.drop('scope', axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_line</th>\n",
       "      <th>label</th>\n",
       "      <th>lemma</th>\n",
       "      <th>base</th>\n",
       "      <th>extent</th>\n",
       "      <th>id_text</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>P225065.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>P225065</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>P225126.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>P225126</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>P235262.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>P235262</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>P235262.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>P235262</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>P235262.31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>P235262</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_line label lemma base  extent  id_text  line\n",
       "36    P225065.4   NaN   NaN  NaN     1.0  P225065     4\n",
       "45    P225126.4   NaN   NaN  NaN     2.0  P225126     4\n",
       "88    P235262.4   NaN   NaN  NaN    10.0  P235262     4\n",
       "109  P235262.30   NaN   NaN  NaN     8.0  P235262    30\n",
       "110  P235262.31   NaN   NaN  NaN     4.0  P235262    31"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.extent > 0].head() #sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Expressions\n",
    "A line in a lexical text may contain more than one word. Usually a list is divided into sections by keyword, for instance:\n",
    "\n",
    "| text                \t| translation                      \t|\n",
    "|---------------------\t|----------------------------------\t|\n",
    "| {ŋeš}gigir          \t| chariot                          \t|\n",
    "| {ŋeš}e₂ gigir       \t| chariot cabin                    \t|\n",
    "| {ŋeš}e₂ usan₃ gigir \t| storage box for the chariot whip \t|\n",
    "| {ŋeš}gaba gigir     \t| breastwork of a chariot          \t|\n",
    "\n",
    "In the comparison between different versions of the list the individual words are less interesting than the *entries*, that is: the sequence of words in a single line. In order to look at entries (rather than words), words in an entry are connected by underscores (_). Since in this case all words are in Sumerian, the language designation (sux:) is removed from the field `entry`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_line</th>\n",
       "      <th>label</th>\n",
       "      <th>lemma</th>\n",
       "      <th>base</th>\n",
       "      <th>extent</th>\n",
       "      <th>id_text</th>\n",
       "      <th>line</th>\n",
       "      <th>entry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P117395.2</td>\n",
       "      <td>o 1</td>\n",
       "      <td>ŋešed[key]N</td>\n",
       "      <td>{ŋeš}e₃-a</td>\n",
       "      <td>0.0</td>\n",
       "      <td>P117395</td>\n",
       "      <td>2</td>\n",
       "      <td>ŋešed[key]N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P117395.3</td>\n",
       "      <td>o 2</td>\n",
       "      <td>pakud[~tree]N</td>\n",
       "      <td>{ŋeš}pa-kud</td>\n",
       "      <td>0.0</td>\n",
       "      <td>P117395</td>\n",
       "      <td>3</td>\n",
       "      <td>pakud[~tree]N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P117395.4</td>\n",
       "      <td>o 3</td>\n",
       "      <td>raba[clamp]N</td>\n",
       "      <td>{ŋeš}raba</td>\n",
       "      <td>0.0</td>\n",
       "      <td>P117395</td>\n",
       "      <td>4</td>\n",
       "      <td>raba[clamp]N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P117404.2</td>\n",
       "      <td>o 1</td>\n",
       "      <td>ig[door]N eren[cedar]N</td>\n",
       "      <td>{ŋeš}ig {ŋeš}eren</td>\n",
       "      <td>0.0</td>\n",
       "      <td>P117404</td>\n",
       "      <td>2</td>\n",
       "      <td>ig[door]N_eren[cedar]N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P117404.3</td>\n",
       "      <td>o 2</td>\n",
       "      <td>ig[door]N dib[board]N</td>\n",
       "      <td>{ŋeš}ig dib</td>\n",
       "      <td>0.0</td>\n",
       "      <td>P117404</td>\n",
       "      <td>3</td>\n",
       "      <td>ig[door]N_dib[board]N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_line label                   lemma               base  extent  \\\n",
       "0  P117395.2   o 1             ŋešed[key]N          {ŋeš}e₃-a     0.0   \n",
       "1  P117395.3   o 2           pakud[~tree]N        {ŋeš}pa-kud     0.0   \n",
       "2  P117395.4   o 3            raba[clamp]N          {ŋeš}raba     0.0   \n",
       "3  P117404.2   o 1  ig[door]N eren[cedar]N  {ŋeš}ig {ŋeš}eren     0.0   \n",
       "4  P117404.3   o 2   ig[door]N dib[board]N        {ŋeš}ig dib     0.0   \n",
       "\n",
       "   id_text  line                   entry  \n",
       "0  P117395     2             ŋešed[key]N  \n",
       "1  P117395     3           pakud[~tree]N  \n",
       "2  P117395     4            raba[clamp]N  \n",
       "3  P117404     2  ig[door]N_eren[cedar]N  \n",
       "4  P117404     3   ig[door]N_dib[board]N  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['entry'] = df['lemma']\n",
    "df['entry'] = df['entry'].str.replace(' ', '_')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by Document\n",
    "The `groupby()` function is used to group the data by document. The function `apply(' '.join)` concatenates the text in the `entries` column, separating them with a white space. The Pandas `groupby()` function results in a series, which is then tranformed into a new Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_text</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P117395</th>\n",
       "      <td>ŋešed[key]N pakud[~tree]N raba[clamp]N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P117404</th>\n",
       "      <td>ig[door]N_eren[cedar]N ig[door]N_dib[board]N i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P128345</th>\n",
       "      <td>garig[comb]N_siki[hair]N garig[comb]N_siki-sik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P224980</th>\n",
       "      <td>gigir[chariot]N e[house]N_gigir[chariot]N e[ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P224986</th>\n",
       "      <td>guza[chair]N_anše[equid]N guza[chair]N_kaskal[...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     entry\n",
       "id_text                                                   \n",
       "P117395             ŋešed[key]N pakud[~tree]N raba[clamp]N\n",
       "P117404  ig[door]N_eren[cedar]N ig[door]N_dib[board]N i...\n",
       "P128345  garig[comb]N_siki[hair]N garig[comb]N_siki-sik...\n",
       "P224980  gigir[chariot]N e[house]N_gigir[chariot]N e[ho...\n",
       "P224986  guza[chair]N_anše[equid]N guza[chair]N_kaskal[..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['entry'] = df['entry'].fillna('')\n",
    "entries_df = df[['id_text', 'line', 'entry']]\n",
    "#entries_df = entries_df.dropna()\n",
    "grouped = entries_df['entry'].groupby(entries_df['id_text']).apply(' '.join).reset_index()\n",
    "by_text_df = pd.DataFrame(grouped)\n",
    "by_text_df = by_text_df.set_index('id_text')\n",
    "by_text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for Niek\n",
    "1) What does the symbol \"~\" mean in the lemmatization? For example in row number one of the df above \"~tree\". \n",
    "\n",
    "NV: This means: \"pertains to\" and is used for words of vague or unclear semantics.\n",
    "\n",
    "2) Can you explain what's going on with the \"line\" column? It appears to start at an arbitrary number for each document.  \n",
    "\n",
    "NV: The field `line` is derived directly from the field `id_line`, minus the `id_text` (P-number) element. `Id_line` is a string but `line` is an integer, used to keep the lines in the right order.  \n",
    "\n",
    "3) Why do the first several entries in the DTM start with a number? It looks like these are all words that are unlematized. What do the numbers refer to? Is \"10[na]_na\" different from \"11[na]_na\"?\n",
    "\n",
    "NV: Some of these entries come from P251686 - and they indicate a problem we hadn't seen before. This is a tablet that combines a list of wooden objects with a metrological table. The metrological table shouldn't be here - there are, I believe, a few other such instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Analysis based on order of entries within a section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining sections\n",
    "\n",
    "The first step in analyzing order of entries within a section must be defining what constitutes a section. We've discussed three methods for defining sections. \n",
    "\n",
    "**1) Expert definition.**  \n",
    "**2) Automated based on composite text (eg. Q000039).**  \n",
    "**3) Automated based on entry proximity.** \n",
    "\n",
    "These methods are described more below:\n",
    "\n",
    "\n",
    "**1) Expert definition.**  \n",
    "Expert manually determines section boundaries based on knowledge of text type. This approach is the most sound, but does not scale. We would not be able to use our workflow with other collections of lexical lists unless without a time-intensive manual step. This method will be set aside for now. We may, however, want to leave users (assuming there ever are any) the option to read in and use their own section definitions for downstream analyses.  \n",
    "\n",
    "**2) Automated based on composite text.**  \n",
    "A composite text is read in and breakpoints in the text are determined based on fuzzy matching of similar words (either in base or lemma). Entries between those breakpoints are assumed to belong to the same section. This method would need to be tested and perhaps supervised to ensure that nonsense sections (a collection of words that don't really belong to any section) aren't grouped together. It may also miss sections that are based not on similarity of words (e.g. \"palm\") but similarity of object type or use (e.g. \"bowl\", \"spoon\", \"cup\"), unless that section is between two sections that are picked up by this method.  \n",
    "\n",
    "Based on the following (artificial) Q text, this method should lead to three sections. Lines 1-4 (related to palm), lines 5-8 (related to polar), and lines 9-14 (related to tree). \n",
    "\n",
    "*1) ŋešnimbar[palm]N*  \n",
    "*2) ŋešnimbar[palm]N sux:suhuš[offshoot]N*  \n",
    "*3) deg[collect]V/t sux:ŋešnimbar[palm]N*  \n",
    "*4) niŋkiluh[broom]N sux:ŋešnimbar[palm]N*  \n",
    "*5) asal[poplar]N*  \n",
    "*6) asal[poplar]N sux:kur[mountain]N*  \n",
    "*7) asal[poplar]N sux:dug[good]V/i*  \n",
    "*8) numun[seed]N sux:asal[poplar]N*  \n",
    "*9) ilur[tree]N*  \n",
    "*10) ad[bush]N*  \n",
    "*11) kišig[acacia]N*  \n",
    "*12) kišighar[tree]N*  \n",
    "*13) samazum[tree]N*  \n",
    "*14) peškal[tree]N*  \n",
    "\n",
    "**3) Automated based on entry proximity**  \n",
    "Lines that always apear within a certain (small) distance from each other could be considered to be part of the same section. Sections are defined based on entire corpus, not just a composite text. (eg. moving window, ~6-8 lines, middle in target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2) Automated based on composite text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.,    1.,    2.,   10.,  999.,    8.,    4.,    3.,   15.,\n",
       "          9.,    7.,   20.,    5.,   17.,    6.,   31.,   11.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to pass unknown values (NaN) from Python to R, need to convert\n",
    "# NaNs in character/string columns become 'unknown'\n",
    "# NaNs in numeric columns become 999\n",
    "# Convert both to NA after passing to R\n",
    "\n",
    "df.extent = df.extent.replace(np.nan, 999)\n",
    "df = df.replace(np.nan, 'unknown', regex=True)\n",
    "\n",
    "#df.count()\n",
    "df.extent.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data.frame':\t5115 obs. of  8 variables:\n",
       " $ id_line: Factor w/ 5115 levels \"P117395.2\",\"P117395.3\",..: 1 2 3 4 5 6 7 8 9 10 ...\n",
       " $ label  : Factor w/ 3006 levels \"1\",\"1'\",\"10\",..: 1777 1799 1804 1777 1799 1804 1777 1799 1804 1818 ...\n",
       " $ lemma  : chr  \"ŋešed[key]N\" \"pakud[~tree]N\" \"raba[clamp]N\" \"ig[door]N eren[cedar]N\" ...\n",
       " $ base   : chr  \"{ŋeš}e₃-a\" \"{ŋeš}pa-kud\" \"{ŋeš}raba\" \"{ŋeš}ig {ŋeš}eren\" ...\n",
       " $ extent : num  0 0 0 0 0 0 0 0 0 0 ...\n",
       " $ id_text: Factor w/ 106 levels \"P117395\",\"P117404\",..: 1 1 1 2 2 2 3 3 3 4 ...\n",
       " $ line   : int  2 3 4 2 3 4 2 3 4 4 ...\n",
       " $ entry  : chr  \"ŋešed[key]N\" \"pakud[~tree]N\" \"raba[clamp]N\" \"ig[door]N_eren[cedar]N\" ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R -i df\n",
    "# bring df into R\n",
    "\n",
    "# convert lemma, base and entry columns to strings\n",
    "df$lemma = as.character(df$lemma)\n",
    "df$base = as.character(df$base)\n",
    "df$entry = as.character(df$entry)\n",
    "str(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data.frame':\t778 obs. of  8 variables:\n",
       " $ id_line: Factor w/ 5115 levels \"P117395.2\",\"P117395.3\",..: 4338 4449 4560 4671 4782 4893 5004 5094 5105 4339 ...\n",
       " $ label  : Factor w/ 3006 levels \"1\",\"1'\",\"10\",..: 1 126 246 368 487 617 732 733 734 756 ...\n",
       " $ lemma  : chr  \"taškarin[boxwood]N\" \"esi[tree]N\" \"ŋešnu[tree]N\" \"halub[tree]N\" ...\n",
       " $ base   : chr  \"{ŋeš}taškarin\" \"{ŋeš}esi\" \"ŋeš-nu₁₁\" \"{ŋeš}ha-lu-ub₂\" ...\n",
       " $ extent : num  0 0 0 0 0 0 0 0 0 0 ...\n",
       " $ id_text: Factor w/ 1 level \"Q000039\": 1 1 1 1 1 1 1 1 1 1 ...\n",
       " $ line   : int  1 2 3 4 5 6 7 8 9 10 ...\n",
       " $ entry  : chr  \"taškarin[boxwood]N\" \"esi[tree]N\" \"ŋešnu[tree]N\" \"halub[tree]N\" ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "# Extract only lines from composite document (in our case Q000039). \n",
    "\n",
    "df_composite = df[which(df$id_text == \"Q000039\"),]\n",
    "df_composite$id_text = droplevels(df_composite)$id_text\n",
    "str(df_composite)\n",
    "table(df_composite$extent)\n",
    "write.csv(df_composite, \"Qtext.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#if(document1 == document2) {\n",
    "#dist = pos2 - pos1 + sum(missing1:missing2)}\n",
    "#else(invalid query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Analysis based on presence/absence of entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Term Matrix\n",
    "Transform the DataFrame into a Document Term Matrix (DTM) by using CountVectorizer. This function uses a Regular Expression (token_pattern) to indicate how to find the beginning and end of token. In the current Dataframe entries are separated from each other by a white space. The expression `r.[^ ]+` means: any combination of characters, except the space.\n",
    "\n",
    "The output of the CountVectorizer (`dtm`) is not in a human-readable format. It is transformed into another DataFrame, with `id_text` as index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(analyzer='word', token_pattern=r'[^ ]+')\n",
    "dtm = cv.fit_transform(by_text_df['entry'])\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), columns = cv.get_feature_names(), index = by_text_df.index.values)\n",
    "dtm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the DTM\n",
    "Each document in the DTM may be understood as a vector, which allows for various kinds of computations, such as distance or cosine-similarity. \n",
    "\n",
    "It is important to recall that the DTM does not preserve information about the order of entries.\n",
    "\n",
    "It is also important to realize that the documents in this analysis of are of very different length (from 1 to 750 entries), with more than half of the documents 3 lines or less. The composite text from Nippur is by far the longest document and will dominate any comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_length = dtm_df.sum(axis=1)\n",
    "df_length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I'll be doing some analysis in R, whereas Niek will be doing some in Python. We can use both languages in different cells of the same notebook and even pass variables between languages. See tip #21 [here](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/).\n",
    "\n",
    "FYI - if you have difficulty running R cells in a Python notebook using rpy2, try installing\n",
    "rpy2 through conda instead of through pip.  \n",
    "\n",
    "`conda install -c r rpy2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure the R correctly parsed the DTM  \n",
    "- It looks like R doesn't allow variable names to start with a number, thus all entries starting with a number had \"X\" added to the beggining of the entry name.  \n",
    "- R doesn't allow parentheses in variable names, so entries like \"1(ban₂)[na]na\" parsed as \"X1.ban...na.na\".   \n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R -i dtm_df\n",
    "#Import dtm_df from Python\n",
    "\n",
    "# Set cols and rows to sum to not include summary column and row added later\n",
    "# This needs to be in separate cell from addition of summary col and row!\n",
    "cols_to_sum = ncol(dtm_df)\n",
    "rows_to_sum = nrow(dtm_df)\n",
    "\n",
    "#head(dtm_df[,1:10])\n",
    "#str(dtm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check density of DTM\n",
    "\n",
    "Look at distribution of document lengths (number of entries per document).  \n",
    "Look at distribution of entry freqency (number of documents each entry appears in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "dtm_df$num_entries = rowSums(dtm_df[1:cols_to_sum])\n",
    "dtm_df[\"num_occurances\",] = colSums(dtm_df[1:rows_to_sum,])\n",
    "dtm_df[\"num_occurances\",\"num_entries\"] = NA \n",
    "\n",
    "plot(density(dtm_df$num_entries, na.rm = TRUE))\n",
    "table(dtm_df$num_entries, useNA = \"ifany\") #number of documents with each number of entries\n",
    "\n",
    "print(paste(\"There are\", length(which(dtm_df$num_entries >= 10)), \"documents with 10 or more entries.\"))\n",
    "print(paste(\"There are\", length(which(dtm_df$num_entries >= 100)), \"documents with 100 or more entries.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "\n",
    "num_occurances = unlist(dtm_df[\"num_occurances\",])\n",
    "plot(density(num_occurances, na.rm = TRUE))\n",
    "table(num_occurances, useNA = \"ifany\")\n",
    "\n",
    "rare = round(length(which(dtm_df[\"num_occurances\",] <=2))/cols_to_sum*100,2)\n",
    "common = length(which(dtm_df[\"num_occurances\",] >=10))\n",
    "most_common = max(dtm_df[\"num_occurances\",], na.rm = TRUE)\n",
    "most_common_entry = colnames(dtm_df[which(dtm_df[\"num_occurances\",] == most_common)])\n",
    "\n",
    "print(paste0(rare, \"% of entries appear only once or twice across the corpus\"))\n",
    "print(paste(common, \"entries occur in 10 or more documents\"))\n",
    "print(paste(\"including one that occurs\", most_common, \"times across the\", rows_to_sum, \"documents\"))\n",
    "print(paste(\"The most common entry is\", most_common_entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "# Look at some of the most common entries\n",
    "colnames(dtm_df)[which(dtm_df[\"num_occurances\",] >= 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "\n",
    "# currently \"variables\" (entries) are sorted alphabetically, would like sorted by frequency\n",
    "dtm_df = as.matrix((dtm_df > 0) + 0) # Converts to binary presence/absence information\n",
    "dtm_df = dtm_df[,order(colSums(dtm_df), decreasing = TRUE)]\n",
    "dtm_df = as.data.frame(dtm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "\n",
    "# Need to recaculate number of occurances, as was converted to binary. \n",
    "dtm_df$num_entries = rowSums(dtm_df[1:cols_to_sum])\n",
    "dtm_df[\"num_occurances\",] = colSums(dtm_df[1:rows_to_sum,])\n",
    "dtm_df[\"num_occurances\",\"num_entries\"] = NA \n",
    "\n",
    "num_occurances = unlist(dtm_df[\"num_occurances\",])\n",
    "most_frequent = max(num_occurances, na.rm = TRUE)\n",
    "most_frequent_entry = colnames(dtm_df[which(dtm_df[\"num_occurances\",] == most_frequent)])\n",
    "\n",
    "print(paste(table(num_occurances)[1], \"entries appear in only one document\"))\n",
    "print(paste(\"The entry that appears in the most documents is\", most_frequent_entry))\n",
    "\n",
    "table(num_occurances, useNA = \"ifany\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "# These entries appear in at least 10 different documents.\n",
    "colnames(dtm_df)[which(dtm_df[\"num_occurances\",] >=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "dtm_df$document = row.names(dtm_df) #add document names as row names\n",
    "dtm_df = dtm_df[-which(row.names(dtm_df) == \"num_occurances\"),] # remove num_occurances row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#install.packages(\"reshape\")\n",
    "library(reshape)\n",
    "\n",
    "melted_dtm_df = melt(dtm_df)\n",
    "head(melted_dtm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "# http://stackoverflow.com/questions/10397183/heat-map-of-binary-data-using-r-or-python\n",
    "#install.packages(\"ggplot2\")\n",
    "library(ggplot2)\n",
    "# ggplot(data = melted_dtm_df[150000:160474,], aes(y=document, x=variable, fill=value)) + \n",
    "#   geom_tile() +\n",
    "#   theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 5))\n",
    "\n",
    "#qplot(data=melted_dtm_df, x=variable,y=document, fill=factor(value),\n",
    "#    geom=\"tile\")+scale_fill_manual(values=c(\"0\"=\"lightblue\", \"1\"=\"red\")) +\n",
    "#  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 1), axis.text.y = element_text(size = 3))\n",
    "\n",
    "# Look at a subset\n",
    "# qplot(data = melted_dtm_df[1:10000,], x=variable, y=document, fill=factor(value),\n",
    "#     geom=\"tile\")+scale_fill_manual(values=c(\"0\"=\"lightblue\", \"1\"=\"red\")) +\n",
    "# theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8), axis.text.y = element_text(size = 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Grouping Documents by Entry Similarity  \n",
    "We can use hierarchical clustering with our presence/absence matrix to uncover groups of similar documents. Ideally, we can benchmark these clusters' accuracy in uncovering geographically or chronologically related documents by looking at metadata, but for this collection the metadata may be too sparse to do that benchmarking.  \n",
    "\n",
    "In either case, we can establish a workflow for doing hierarchical clustering and apply that to other datasets with better provenance information to test for cluster utility.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troubleshooting note:  \n",
    "When running R as a magic within Jupyter notebook, running install.packages() leads to the notebook prompting you for a selection. It turns out that this is due to the fact that anaconda actually installs a second R installation and stores installed packages separately from the users \"main\" R installation.  \n",
    "\n",
    "To avoid this issue, run '.libPaths()' within R in your console to find the path where anaconda stores your packages. You can then download binaries from CRAN and put them in that directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%R \n",
    "# See Troubleshooting note above\n",
    "\n",
    "#install.packages(\"ggdendro\")\n",
    "# install.packages(\"ggdendro\", \"/anaconda/lib/R/library\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(ggdendro)\n",
    "clusters <- hclust(dist(dtm_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "ggdendrogram(clusters, rotate = TRUE) + theme(axis.text.y = element_text(size = 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding provenience by using ORACC metadata\n",
    "The file `data/metadata/dcclt-eta.csv` contains some metadata, including document provenience when known. We read in this data and add provenience to our DTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "# Bring in metadata\n",
    "ids = read.csv(\"../data/metadata/dcclt_meta.csv\")\n",
    "ids$document = ids$X\n",
    "ids$X = NULL\n",
    "head(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "# Add provenance information to dtm_df\n",
    "dtm_df = merge(dtm_df, ids, by = \"document\")\n",
    "dtm_df$provenience = droplevels(dtm_df)$provenience\n",
    "table(dtm_df$provenience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Add colors to dendrogram by provenance\n",
    "library(scales)\n",
    "numColors = length(levels(factor(dtm_df$provenience)))\n",
    "numColors\n",
    "myPalette = brewer_pal(palette = \"Paired\")(numColors)\n",
    "names(myPalette) = levels(dtm_df$provenience)\n",
    "print(names(myPalette))\n",
    "show_col(myPalette)\n",
    "ggdendrogram(clusters, rotate = TRUE) + \n",
    "theme(axis.text.y = element_text(size = 8, color = myPalette[dtm_df$provenience]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
